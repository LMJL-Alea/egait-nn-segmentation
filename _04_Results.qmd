# Results {#sec-results}

Various models are built and evaluated to find the best model for each strategy. Then, both strategies are compared to select the best one and have a resulting final model.

## Tuning of hyper-parameters

In order to choose the best model for each strategy, we tuned the hyper-parameter of each model for both strategies. Since we do not use the same metric to tune and evaluate models for each strategy, as we use the GWYI in the strategy E and accuracy in the strategy P, we cannot directly choose a strategy at this step. Therefore, we select one model per strategy and compare the two strategies later.

We choose a set of values to test for each hyper-parameter of the feature space tuning (see @tbl-grid-fs). We recall that the hyper-parameters *spar* and *n_lag* are used in both strategies but the hyper-parameter *k* is used only in the strategy E to label events.

::: {#tbl-grid-fs tbl-pos="H"}

```{r}
tibble::tibble(
  param = c("spar", "n_lag", "k"),
  description = c(
    "Smoothing curve parameter",
    "Kept times in the past",
    "Number of labeled points"
  ),
  strategy = c("Both", "Both", "Strategy E"),
  values = c("0.3, 0.4, 0.5, 0.6, 0.7", "1, 2, 3, 4, 5", "1, 2, 3")
) |>
  gt::gt() |>
  gt::cols_label(
    param = "Parameter",
    strategy = "Strategy",
    description = "Description",
    values = "Tuning Grid"
  ) |>
  gt::cols_align(
    align = "left",
    columns = values
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = param)
  ) |>
  gt::tab_options(column_labels.background.color = "#616161")
```

Feature space parameters tuning grid values.

:::

For the strategy E, we also tune the weight classes used to address the class imbalance issue. For this matter, we fix the weight of the class *None* with the value 1, and we test different weights for the classes of interest (see @tbl-grid-class-weights). For the strategy P, as the class imbalance is less present, we only use the weights from the inverse class frequency method defined in @eq-inv-class-frequency.

::: {#tbl-grid-class-weights tbl-pos="H"}

```{r}
is_empty <- function(x) {
  return(x == "")
}

options(gt.html_tag_check = FALSE)
tibble::tibble(
  param = c("Weight on class *None*", "Weight on event classes"),
  values = c("1", "40, 60, 80")
) |>
  gt::gt() |>
  gt::cols_label(
    param = "Parameter",
    values = "Tuning Grid"
  ) |>
  gt::tab_options(column_labels.background.color = "#616161") |>
  gt::fmt_markdown(columns = param)
```

Strategy E: Class weight tuning grid values.

:::

Finally, we build tuning grid values for every model hyper-parameters which are common to both strategies (see @tbl-grid-model).

::: {#tbl-grid-model tbl-pos="H"}

```{r}
tibble::tibble(
  model = c(
    "Multinomial Regression",
    "Multinomial Regression",
    "Decision Tree",
    "Decision Tree",
    "Bagged Trees",
    "Bagged Trees",
    "Random Forest",
    "Random Forest",
    "Boosted Trees",
    "Boosted Trees",
    "Boosted Trees",
    "Boosted Trees",
    "KNN",
    "KNN",
    "Neural Network",
    "Neural Network",
    "Neural Network"
  ),
  param = c(
    "`penalty`",
    "`mixture`",
    "`tree_depth`",
    "`min_n`",
    "`tree_depth`",
    "`min_n`",
    "`trees`",
    "`min_n`",
    "`trees`",
    "`min_n`",
    "`tree_depth`",
    "`learn_rate`",
    "`neighbors`",
    "`dist_power`",
    "`hidden_units`",
    "`learn_rate`",
    "`dropout`"
  ),
  description = c(
    "Regularization parameter",
    "Regularization type (Lasso, Ridge, Elastic Net)",
    "Tree depth",
    "Minimal number of observations before splitting",
    "Tree depth",
    "Minimal number of observations before splitting",
    "Number of trees",
    "Minimal number of observations before splitting",
    "Number of trees",
    "Minimal number of observations before splitting",
    "Tree depth",
    "Learning rate",
    "Number of nearest neighbors",
    "Minkowski distance order",
    "Hidden layers and units",
    "Learning rate",
    "Rate of randomly deactivated nodes"
  ),
  values = c(
    "0.01, 0.001, 0.0001",
    "0, 0.25, 0.5, 0.75, 1",
    "1:15",
    "2:40",
    "1:15",
    "2:40",
    "100, 300, 500, 700, 900",
    "2:40",
    "400, 600, 800, 1000",
    "20, 40",
    "5, 8, 12",
    "0.01",
    "3, 5, 10, 20",
    "1, 2",
    "hidden layers: 2:5; nodes: 100, 500",
    "0.01, 0.001",
    "0.1, 0.3"
  )
) |>
  dplyr::group_by(model) |>
  dplyr::mutate(model = ifelse(dplyr::row_number() == 1, model, "")) |>
  dplyr::ungroup() |>
  gt::gt() |>
  gt::tab_style(
    style = list(
      gt::cell_borders(
        sides = c("top", "bottom"),
        weight = gt::px(0)
      )
    ),
    locations = list(
      gt::cells_body(
        columns = model,
        rows = is_empty(model)
      )
    )
  ) |>
  gt::cols_label(
    model = "Model",
    param = "Parameter",
    description = "Description",
    values = "Tuning Grid"
  ) |>
  gt::cols_width(
    model ~ "25%",
    param ~ "17%",
    description ~ "38%",
    values ~ "25%"
  ) |>
  gt::tab_options(column_labels.background.color = "#616161") |>
  gt::fmt_markdown(columns = param)
```

Model hyper-parameter tuning grid values.

:::

For each strategy and each model, we get the optimal parameters thanks to tuning (see @tbl-tuning-res-events and @tbl-tuning-res-phases in the Appendix for the detail of chosen parameters for each model). We then evaluate each model on the test set which was never seen before to find the best model for each strategy (see @tbl-res).

::: {#tbl-res tbl-pos="H"}

```{r}
data.frame(
  model = c(
    "Multinomial Regression",
    "Decision Tree",
    "Bagged Trees",
    "Random Forest",
    "Boosted Trees",
    "KNN",
    "Neural Network"
  ),
  youden = c("67.6%", "60.0%", "70.3%", "70.5%", "83.0%", "24.0%", "88.7%"),
  accuracy = c("82.7%", "73.9%", "74.9%", "89.4%", "89.7%", "88.0%", "89.4%")
) |>
  gt::gt() |>
  gt::cols_label(
    model = "Model",
    youden = "Strategy E: GWYI",
    accuracy = "Strategy P: accuracy"
  ) |>
  gt::tab_options(column_labels.background.color = "#616161") |>
  gt::tab_style(
    style = gt::cell_fill(color = 'lightgreen'),
    locations = gt::cells_body(columns = c(youden), rows = 7)
  ) |>
  gt::tab_style(
    style = gt::cell_fill(color = 'lightgreen'),
    locations = gt::cells_body(columns = c(accuracy), rows = 5)
  ) |>
  gt::cols_width(
    everything() ~ px(200)
  )
```

Performance metrics on the test set for both strategies.

:::

We keep one model per strategy to maximize the performance metric evaluated on the test set. For the strategy E, we select the Neural Network model to maximize the GWYI. For the strategy P, we select the Boosted Trees model to maximize accuracy. We can note that for the strategy P, three models (the random forest, the boosted trees and the neural network) had close results between 89.4% and 89.7%. However, since all models had really quick computing times to make predictions on the test set, this was not a criteria to choose one model over another one, so we could simply choose the model giving the highest score.

Now that we selected a model for each strategy, the next step is to compare the two strategies to choose the best one, leading to a final model.

## Selecting a strategy

To choose a final model, we need to determine the best strategy. In order to do so, we compare the predictions made on the test set by the two previously selected models with the real events given by the gold standard to see which model gives closer results. We still use the test set to evaluate the strategies.

First, we shorten the time-series to retain the times where we want to detect points by keeping only the time range where the gold standard detected the four events perfectly. Indeed, at the start and the end of the session, this tool can miss contact points because the subject is not walking on the mat sensors. At the start of the session, we search for the first *Heel Strike* event which is followed by the three other events in the correct order. At the end of the session, we remove all points after the first missing event.

In both strategies, the model does not detect only one point corresponding to the event but a window of points containing the event (see @fig-preds-both-strategies). This means that we need to build a procedure to get precise estimated event times from the model predictions. In the strategy P, when predicting sub-phases, the event corresponding to the sub-phase is theoretically located at the start of the sub-phase but we see that it is not always the case with our model's predictions. Hence we use the same process for both strategies to choose one point either in the windows detected with the strategy E or in the sub-phases predicted with the strategy P.

::: {#fig-preds-both-strategies layout="[45, -10, 45]" fig-pos="H"}

![Strategy E: Predictions made by the neural network.](images/preds_events_and_real_events.png){#fig-preds-events width=270}

![Strategy P: Predictions made by the boosted trees.](images/preds_phases_and_real_events.png){#fig-preds-phases width=270}

Predictions from both strategies. The bigger and darker points represent the real time of events given by the GAITRiteÂ© gold standard.
:::

In each window or phase detected, the following process is applied to keep one point representing the event. We first need to identify each of them correctly, using the two following parameter:

- A threshold to firstly select only the observations with a high probability of being in the class: $p > 0.4$.
- A time to know when to distinguish two predicted windows or phases: if the first point of the window $i$ is at more than $t = 40 ms$ from the last point of the window $i-1$ we consider the windows distinct. It enables us to define unique windows with points that are not necessarily consecutive when only one event should be detected in this time range.

Then, in each of these clearly separated windows or phases, we keep the point having the highest probability of being in this class according to the model. This point is the time of the estimated event. The last step is to compare these estimated events with the ones given by the gold standard to see which of the strategy gives better result and select a final strategy.

The first point of comparison is the number of each event detected during the walking sessions. For both strategies, the model predicts the exact number of events for every classes in each session. It means that both models seem to perform well but it does not allow us to tell them apart. The second item that can be compared is the time at which the events are happening. We plot the mean time difference between real and predicted events for each session in the test set (see @fig-boxplot-comparison).

![Mean time difference between real and predicted events for both strategies.](images/boxplot_both_strategies.png){#fig-boxplot-comparison width=600}

In the strategy P, the *Heel-Strike* events seem to be well estimated but it not the case for the *Toe-Off* events that are predicted too late, about 150 ms after the real time of the event. This could mean that for these events, it would give better results to take one of the first points of the detected sub-phase instead of the point with the model highest probability. Since we do not have theoretical explanation of this phenomena, we can not justify the use of different procedure for either *Heel Strike* or *Toe-Off* events. On the other hand, in the strategy E, all of the event times are estimated near the real ones with no distinction between classes. For each type of event, the estimated time median is at approximately 50 ms before the real event, with a few outliers. Overall, the results given by the event detection are closer to the times given by the gold standard so we retain the strategy E. 

\newpage

## Detailed performance of the selected model

The final selected model is the neural network from the strategy E predicting directly the event times. We give a more in-depth evaluation of the final model in this part. We evaluate this model on the test set for the three metrics adapted to this strategy: the Macro-Average Sensitivity and Specificity and the GWYI (see @tbl-final-eval). We also perform a deeper comparison of the estimated event times by the model with the real ones given by the gold standard.

::: {#tbl-final-eval tbl-pos="H"}

```{r}
tibble::tibble(
  youden = c("88.7%"),
  sen = c("93.4%"),
  spe = c("96.4%")
) |>
  gt::gt() |>
  gt::cols_label(
    youden = "Generalized Weighted Youden Index",
    sen = "MA Sensitivity",
    spe = "MA Specificity"
  ) |>
  gt::tab_options(column_labels.background.color = "#616161")
```

Performance metrics of the selected Neural Network on the test set.

:::

For a complete comparison of our model with the gold standard, we use spatio-temporal parameters that can be computed from the four events happening during a walking cycle: the number of cycles in a session, the cycle duration and the percentage of stance phase. We compute the mean value for these parameters on each session from the test set. We differentiate the results for each foot by computing the gait cycles starting with the right and the left foot, thus we have 70 observations for this comparison (1 per session and foot in the test set). 

For the number of cycles in each session, we find the exact same number of cycles for our model and the gold standard, which is not surprising since we previously shown that the model detected the correct number of each event in every sessions. For the mean cycle duration and the mean percentage of stance phase, we use Bland-Altman plots [@bland_altman] to compare the the results (see @fig-bland-altman).

::: {#fig-bland-altman layout="[48, -4, 48]" fig-pos="H"}

![Comparison of the mean cycle duration (seconds).](images/ba_cycle_length.png){#fig-ba-cycle-length width=270}

![Comparison of the mean stance percentage (%).](images/ba_stance_percent.png){#fig-ba-stance-percent width=270}

Bland-Altman plots to compare our model with the gold standard on spatio-temporal parameters.
:::

For both parameters, the bias represented by the dashed line in the blue interval is really close to zero, meaning that our model gives similar results than the gold standard. More precisely, for the mean cycle duration, the differences between the two methods are between minus 26 ms and 21 ms, represented by the dashed lines in the red and green intervals. Since a gait cycle typically lasts one second, this difference interval seems low enough to consider our method to be comparable to the gold standard. For the mean stance percentage, the differences are between minus 8.50% and 6.5% and we know that a stance phase generally lasts about 60% of the gait cycle. Once again our model gives coherent results that are close to the parameters given by the reference method.