# Introduction

## Context

### Gait analysis

Study of the human gait has been shown to be important in various health applications, such as study of general health in elderly populations [@beauchet2016poor]. Wearable sensors are increasingly used for gait analysis as they are smaller, cheaper and more convenient than other methods such as motion capture systems or mats containing pressure sensors.

A walking cycle is defined as a stride, which is composed of two steps. More formally, a stride is the set of movements performed between two consecutive contacts of the heel of the same foot with the ground. The goal of our method is to detect different events happening during gait cycles when a subject is walking. This work can be used to segment the gait as cycles to extract strides and it allows further study of the gait such as spatio-temporal parameters analysis.

More precisely, the detected events are the moments where a foot touches or leaves the ground. They are called Heel-Strike and Toe-off respectively and they can be used to segment the stride in phases. The stance phase corresponds to the moments when the foot is in contact with the ground and can be found between the Right Heel-Strike and Left Toe-off events when a cycle starts with the right foot (see @fig-walking-cycle). This phase lasts generally 60% of the entire stride and is followed by the swing phase when the right foot is not anymore in contact with the ground.

With these four markers identified during each cycle, the gait can be analysed with parameters such as stride length and stance phase percentage variability or symmetry between feet.


![Events and phases of a typical gait cycle. [^jacquelin-perry]](images/walking-cycles.png){#fig-walking-cycle width=400}

[^jacquelin-perry]: Figure annotated from Jaquelin Perry's one on [Wikipedia](https://commons.wikimedia.org/wiki/File:GaitCycle_by_JaquelinPerry.jpg).


### Unit quaternions

Unit quaternions can be used to represent the 3D rotation of an object over time [@hamilton1844;@voight2021quaternion]. This representation has several advantages, as it is a rather compressed representation containing only four values and avoiding the gimbal lock problem presents in other representations. Unit quaternions were therefore chosen as the data type returned by the sensor.

Quaternions are four-dimensional vectors denoted as $\mathbf{q} = \left( q_w, q_x, q_y, q_z \right)$, but can also be viewed as hypercomplex numbers of rank 4. Unit quaternions have a norm of 1 and can encode a 3D rotation with a rotation angle $\theta \in [0, 2\pi]$ and a rotation axis $\mathbf{u} = (u_x, u_y, u_z) \in S^2$, where $S^2$ is the 2-sphere, using the following formula:

$$
\mathbf{q} = \text{cos}\left(\frac{\theta}{2}\right) + \left(u_x i + u_y j + u_z k \right) \text{sin}\left(\frac{\theta}{2}\right)
$$ {#eq-quaternions}

with :

- $i$, $j$, and $k$ generalizing the imaginary number $i$, as $i^2=j^2=k^2=ijk=-1$.
- $||\mathbf{q}|| = \sqrt{\mathbf{qq}^t} = 1$.

The set of unit quaternions, denoted $\mathbb{H}_u$, possesses interesting properties. The quaternions $\mathbf{q}$ and $-\mathbf{q}$ represent the same rotation. The group is equipped with an identity element $\mathbf{q}^{(0)} = (1,0,0,0)$ which corresponds to the identity rotation, such that $\mathbf{q} \mathbf{q}^{(0)} = \mathbf{q}^{(0)}\mathbf{q} = \mathbf{q}$.

It is possible to use the geodesic distance $d_g$ between two quaternions $\mathbf{q}_1$ and $\mathbf{q}_2$ to define a metric space $(\mathbb{H}_u, d_g)$, with:

$$
d_g (\mathbf{q}_1, \mathbf{q}_2) = ||\text{log}(\mathbf{q}_1^{-1} \mathbf{q}_2)||
$$ {#eq-dist-geodesique}

In this application, the sensor orientation is the rotation between the reference frame of origin, here the Earth's reference frame $R_f=(f_1, f_2, f_3)$ and its own reference frame $R_s=(s_1, s_2, s_3)$ formed by the accelerometer, gyroscope, and magnetometer (see @fig-sensor-axis).

![Sensor reference and axis. [@drouin2023semi]](images/sensor-axis.png){#fig-sensor-axis width=200}

Therefore, the IMU returns unit quaternion time-series allowing the study of the 3D hip rotation over time, as four-component vectors.



## State of the art

In the past, several methods for gait cycle segmentation have been studied based on signals recorded from wearable sensors placed on different parts of the body. We try in this part to give an overview of proposed methods by grouping them into categories, with variations related to the position of sensors on the body, the type of sensor measurement system used, and the characteristics of the signal.

Search for feature points: Peak Detection and zero-crossing

: Peak detection algorithms exploit the semi-periodic properties of the gait cycle, assuming that specific events during a gait cycle typically match the maxima or minima of a recorded signal. Several algorithms based on this method have been developed. Spatio-temporal gait parameters have been determined from the acceleration signal of a sensor by using peak detection and zero-crossing methods to identify stride cycles [@zijlstra2003assessment]. Some peak detection algorithms have been developed to work on any kind of signal [@jiang2017robust] or are specifically implemented to work to detect gait events on a signal, often Heel-Strike on acceleration data [@lueken2019peak]. Most methods detect Heel-Strike and Toe-off events by identifying minima, maxima or zero-crossing of acceleration time series [@gonzalez2010real;@mariani2013quantitative;@panebianco2018analysis]. These events can be called Initial Contact and Terminal Contact and have also been identified on the foot inclination angle in the sagittal plane [@nazarahari2022foot].

Analysis of dynamic properties: Wavelet Transform

: To detect gait events in recorded signals, another used method is wavelet transform because it allows detection of a specified frequency at a specified time. This method has been used to detect Heel-Strike and Toe-off events on an angular velocity signal by using multi-resolution wavelet decomposition [@aminian2002spatio]. The signal is decomposed into wavelet packages, using high-scale and low-scale filters. In @mccamley2012enhanced, the signal of the vertical acceleration is smoothed by integrating and then differentiating using a Gaussian Continuous Wavelet Transform (CTW). Events are found on minima and maxima of the differentiated signal. Furthemore, a method called Sparsity-assisted wavelet denoising (SWAD) has been developed to segment the signal in three events: midstance, toe-off and heel-strike [@prateek2019gait]. It uses a combination of linear time invariant filters, wavelets and sparsity based methods to extract a coefficient vector of Discrete Wavelet Transform (DTW). That generates a sparse template of moving segments of gyroscope measurements.

Pattern-matching: Dynamic Time Warping (DTW)

: To segment signals, another method is pattern-matching. Dynamic Time Warping is widely used for this matter. It allows identification of patterns with different length and it matches signals non-linearly. Thus it is commonly used to evaluate the similarity between time series. This method has been used on an acceleration signal to identify gait cycles [@ghersi2020gait]. In another study, multi-dimensional subsequence DTW (msDTW) was used to segment strides using informations from different axes of an accelerometer and a gyroscope at the same time [@barth2015stride].

Hidden Markov Models

: Hidden Markov Models (HMMs) are machine learning models that can be used to simultaneously segment and classify data. In gait analysis, hidden states of the HMM are viewed as activity classes or gait phases. Continuous HMM with Gaussian mixture model are often used to model the outputs. Each cycle can be modeled with a circular left to right HMM, included in a more global HMM model to classify walking activities [@panahandeh2013continuous]. Other algorithms have the goal of detecting four gait events which are Heel-Strike, Flat-Foot, Heel-Off and Toe-Off. They use a four state left to right HMM with observations following a multivariate Gaussian distributions [@mannini2011hidden;@garcia2022adaptive]. Hierarchical HMMs have shown to perform better than Peak detection and Dynamic Time Warping algorithms on walking data [@haji2018segmentation].

Other Machine Learning methods

: Many studies on gait analysis use wearable sensors and machine learning. A study showed that most used methods for this purpose are SVM and CNN, and are largely used to detect diseases or recognize activities when analyzing gait data [@saboor2020latest]. SVM divides a set of objects into classes with widest possible margin at the hyperplane boundaries. Neural Networks use interconnected nodes organized in layers to process informations and learn patterns through back-propagation and make predictions using activation functions.

: Studies show good results when using machine learning to classify gait of patients having trouble walking, for instance patients with Parkinson Disease [@tahir2012parkinson;@wahid2015classification], or to extract gait parameters [@rampp2014inertial;@hannink2016sensor]. These studies mostly use Artificial Neural Networks but do not address the problem of the segmentation of the signal recorded by the sensors. Similarly, in @farah2019design, a logistic regression tree is used to classify gait phases in a stride but not to directly segment the signal into strides.

: For the segmentation matter, Recurrent Neural Networks have shown good results for identifying Heel-Strike and Toe-off events with pressure sensors, accelerations and Euler angles [@prado2019gait]. Another study compared unsupervised machine learning (k-means) with supervised one (SVM and CNN), showing that CNNs were performing the best to predict stance and swing phases [@potluri2019machine]. Lastly, in another study, 24 time series from three sensors’ accelerometers and gyroscopes were used as input into a 6 layers CNN to estimate the likelihood of the corresponding input sample, given a specific gait event [@gadaleta2019deep]. For instance, the signal can be segmented with the initial contact of the right foot if this gait event is chosen in the model. This method was shown to perform better than a Wavelet Transform algorithm.

In our research, we did not find studies using a decision tree model to segment the sensor signal into strides.


# Proposed segmentation model

## Data acquisition

A wearable sensor was used to record the hip rotation. It contains an acceleromerer, a gyroscope and a magnetometer. Subjects were wearing the sensor on their right hip (see @fig-sensor-position). The frequence of the sensor is 100Hz. With this device, the data acquired is in the form of quaternion time-series, representing the 3D rotation of the hip over time.

![Sensor positionned on the right hip.](images/sensor-position.png){#fig-sensor-position width=150}


Furthermore, during acquisitions, subjects were walking on the GAITRite© mat, a gold standard in gait analysis [@Menz2004]. It implies that subjects were walking approximately nine meters on a straight line. This device gives various information thanks to pressure sensors contained in the mat such as the times where the subject feet touche and leave the ground at each step, meaning that these times can be used to know when the gait events actually happened. This data is then used to train the model. To use the two devices simultaneously, they were started at the same time by the same person with their two indexes, allowing a good synchronization between devices.

Six subjects have been included in this study, each of them walking at different speeds (see @tbl-subjects). 

<!-- +-----+--------+-------+-------------------------+ -->
<!-- | ID  | Gender | Age   | Speed (cm/s)            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | MBA | M      | 50-60 | - Slow : 60             | -->
<!-- |     |        |       | - Intermediate : 116    | -->
<!-- |     |        |       | - Normal : 145          | -->
<!-- |     |        |       | - Fast : 199            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | MBO | F      | 20-30 | - Slow : 73             | -->
<!-- |     |        |       | - Intermediate : 122    | -->
<!-- |     |        |       | - Normal : 145          | -->
<!-- |     |        |       | - Fast : 188            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | MSI | F      | 20-30 | - Slow : 67             | -->
<!-- |     |        |       | - Intermediate : 115    | -->
<!-- |     |        |       | - Normal : 148          | -->
<!-- |     |        |       | - Fast : 179            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | MTR | M      | 20-30 | - Slow : 77             | -->
<!-- |     |        |       | - Normal : 132          | -->
<!-- |     |        |       | - Fast : 185            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | NNE | F      | 20-30 | - Slow :  57            | -->
<!-- |     |        |       | - Intermediate : 116    | -->
<!-- |     |        |       | - Normal : 147          | -->
<!-- |     |        |       | - Fast : 190            | -->
<!-- +=====+========+=======+=========================+ -->
<!-- | TDE | M      | 50-60 | - Slow : 61             | -->
<!-- |     |        |       | - Normal : 120          | -->
<!-- |     |        |       | - Fast : 193            | -->
<!-- +-----+--------+-------+-------------------------+ -->

<!-- : Summary of subjects and walking speeds used for the model. {#tbl-subjects} -->


::: {#tbl-subjects tbl-pos="H"}

```{r}
data.frame(
  id = c("MBA", "MBO", "MSI", "MTR", "NNE", "TDE"),
  gender = c("M", "F", "F", "M", "F", "M"),
  age = c("50-60", "20-30", "20-30", "20-30", "20-30", "50-60"),
  slow = c(60, 73, 67, 77, 57, 61),
  intermediate = c(116, 122, 115, NA, 116, NA),
  preferential = c(145, 145, 148, 132, 147, 120),
  fast = c(199, 188, 179, 185, 190, 193)
) |> 
  gt::gt() |> 
  gt::tab_spanner(
    label = "Walking speed (cm/s)",
    columns = c(slow, intermediate, preferential, fast)
  ) |> 
  gt::cols_label(
    id = "ID",
    gender = "Gender",
    age = "Age range (years)",
    slow = "Slow",
    intermediate = "Intermediate",
    preferential = "Preferential",
    fast = "Fast"
  ) |> 
  gt::sub_missing() |> 
  gt::opt_stylize(style = 6, color = 'gray') |> 
  gt::cols_align(align = "center") |> 
  gt::tab_style(
    style = "vertical-align:top", 
    locations = gt::cells_column_labels()
  )
```

Summary of subjects and walking speeds used to train the model.

:::


## Data presentation

Sensor data

: As mentioned before, the wearable sensor returns unit quaternion time-series representing the rotation of the hip over time, allowing visualization of each coordinate time-serie (see @fig-timeserie). It is important to note that on this figure, the walking cycles are clearly apparent and consistent over time, as it represents a healthy gait. This is not the case for subjects having gait disorders, which is one of the reason we develop a method more complex than peak detection. Our method also allows prediction of different events during a walking cycle which would not be allowed by only segmenting the signal into cycles.

![Data returned by the wearable sensor.](images/time-serie.png){#fig-timeserie width=350}

Sensor data preprocessing 

: A centring step is applied on the quaternion time-series to center them around a mean. Supposing we have $n$ time-series $\mathbf{Q}_1, \dots, \mathbf{Q}_n$ on the same time grid $t_1, \dots, t_p$, we can write $\mathbf{Q}_i(t_k) = \mathbf{q}_{ik} \in \mathbb{H}_u$, with $i \in [\![ 1, n ]\!]$ et $k \in [\![ 1, p ]\!]$. We use the Fréchet mean associated to the geodesic distance $d_g$ (see @eq-dist-geodesique) to compute the mean of each quaternion $\mathbf{q}_{1k}, \dots, \mathbf{q}_{nk}$ for each time $t_k$. 

: $$
\mathbf{q}_k^{(m)} = \mathbf{Q}^{(m)} (t_k) = \underset{q \in \mathbb{H}_u}{\mathrm{argmin}} \sum_{i=1}^n d_g^2(\mathbf{q}_{ik}, \mathbf{q}), \hspace{5mm} k \in [\![ 1, p ]\!]
$$ {#eq-mean-qts}

: The centered time-series $\mathbf{Q}_1^{(c)}, \dots, \mathbf{Q}_n^{(c)}$ can then be computed.

: $$
\mathbf{q}_{ik}^{(c)} = \mathbf{Q}_i^{(c)} (t_k) = \left( \mathbf{q}_k^{(m)} \right)^{-1} \mathbf{q}_{ik}, \hspace{5mm} k \in [\![ 1, p ]\!],\hspace{2mm}  i \in [\![ 1, n ]\!]
$$ {#eq-centring-qts}

: The other pre-processing step is to switch to a functional representation using cubic splines to be able to compute derivatives [@ramsay2005].


Pressure mat data

: The GAITRite© mat returns directly spatio-temporal parameters such as stride duration, stride length or speed. Since the two devices were triggered simultaneously, the same time range is used to associate each gait event with a time on the sensor time-series. More precisely, each time of the time-series will be classified between a number of classes. This method rises a class imbalance challenge that will be addressed later in the article.


## Feature space

To implement a machine learning model, we build a feature space containing variables characterizing the hip rotation over time.

Angular velocity and acceleration

: Supposing we can compute first and second derivatives of a quaternion time-series over time, we can compute angular velocity and acceleration [@narayan2017]. The angular velocity $\pmb{\Omega}$ is a vector which has for direction the axis of rotation and for quantity the angular velocity.

: $$
\pmb{\Omega} = 2 \mathbf{q}^{-1} \dot{\mathbf{q}} \hspace{3mm} \text{with} \hspace{3mm} \dot{\mathbf{q}} = \frac{d \mathbf{q}}{dt} = \frac{1}{2} \mathbf{q} \hspace{1mm} \pmb{\Omega}
$$ {#eq-angular-vel}

: Similarly, the angular acceleration $\dot{\pmb{\Omega}}$ is the angular velocity derivative.

: $$
\dot{\pmb{\Omega}} = 2 \left( \mathbf{q}^{-1} \ddot{\mathbf{q}} - (\mathbf{q}^{-1}\dot{\mathbf{q}})^2 \right) \hspace{3mm} \text{with} \hspace{3mm} \ddot{\mathbf{q}} = \frac{d^2 \mathbf{q}}{dt^2} = \frac{1}{2} \left( \dot{\mathbf{q}} \hspace{1mm}  \pmb{\Omega} + \mathbf{q} \hspace{1mm}  \dot{\pmb{\Omega}} \right)
$$ {#eq-angular-acc}

Euler angles

: The angles named Roll, Pitch and Yaw represent rotations around the three principal axis. Their computation is done using the quaternion time-series, with the following rotation matrix to go from the quaternion $\mathbf{q} = (q_w, q_x, q_y, q_z)$ to the angles.

$$
\begin{bmatrix}
\text{Roll} \\
\text{Pitch} \\
\text{Yaw}
\end{bmatrix}
= 
\begin{bmatrix}
\text{atan2} \left(2(q_w q_x + q_y q_z), 1-2(q_x^2 + q_y^2)  \right) \\
\text{asin} \left(2(q_w q_y - q_x q_z) \right) \\
\text{atan2} \left(2(q_w q_z + q_x q_y), 1-2(q_y^2 + q_z^2)  \right)
\end{bmatrix}
$$ {#eq-rpy}


Walking speed

: One of our hypothesis is that the gait can differ depending if the subject walks slowly of faster. Thus this variable was also added to the feature space by getting it from the GAITRite© mat output.

The feature space depends on two hyper-parameters. The first one is a smoothness parameter *spar* for the time-serie curves, as derivation is used to compute some variables. It is contained in $]0,1]$ and is used to fit a cubic smoothing spline to the time-serie. The second one is the *lag* parameter, to keep at the time $t_p$ the information from the time $t_{p-1}, \dots, t_{p-lag}$. That parameter implies that the feature space contains $10 + 9 \times lag$ variables.


## Classification strategies

To segment the gait, we compare two strategies to classify the observations.

Method 1: Predicting gait events

: The first method is to directly predict the gait events happening during the walk. We classify the observations in five classes for each event and other times :

: - Right Heel Strike
- Left Toe Off
- Left Heel Strike
- Right Toe Off
- None (all other times not corresponding to a certain event)

: This strategy is the most direct one as we obtain the event times with the gold standard. However, this classification raises an issue of severe class imbalance with the negative class widely over-represented. Indeed, the class we call *None* that represents times that do not belong to an event is clearly larger than the four other ones.

![Example of events represented on a unit quaternion time serie.](images/events_on_time_serie_MSI_N_R2.png){#fig-event_timeserie width=350}

When predicting events, we choose to label a number of points $k$ as corresponding to the event around the precise event time. This allows to take into account some uncertainty, as the time range between two points is only 10 ms. For instance, by labeling as part of an event three points rather than just one, the event happens in a window of 30 ms. This strategy also reduces somewhat the imbalance between the class proportions.

Method 2: Predicting phases

: The second method consists of predicting phases in the signal that are characterized with the gait events. We label each part in the signal between two events as a phase, leading to four classes for our observations:

: - Pre-Stance phase: between Right Heel Strike and Left Toe Off
- Stance phase: between Left Toe Off and Left Heel Strike
- Pre-Swing phase: between Left Heel Strike and Right Toe Off
- Swing phase: between Right Toe Off and Right Heel Strike

: To name these classes, we base ourselves on a typical cycle starting with the right foot, starting with a stance phase while the right foot is on the ground followed by a swing phase. This classification allows us to have four classes that are less imbalanced without a negative class. The disadvantage is that after the model predictions we need to find the events from the predicted phases.

![Example of phases represented on a unit quaternion time serie.](images/phases_on_time_serie_MSI_N_R2.png){#fig-phases_timeserie width=350}


## Supervised classification models

Multinomial Regression [@hosmer2013applied; @hastie2015statistical]

: A multinomial regression is built by fitting a set of independent binary logistic regressions. Let $N$ be the number of observations and $X = (X_1, \dots, X_p)$ the predictors. We want to classify our observations in $K$ classes. We choose one category to be the baseline, we name it $k_0$. We build $K-1$ logit equations modeling the log-odds of a class $k$ versus the class $k_0$. The logit equation for a class $k$ is:

: $$
log\left( \frac{\mathbb{P}(Y = k | X)}{\mathbb{P}(Y = k_0 | X)} \right) = \beta_{0k} + \beta_k^TX
$${#eq-logit}

: with $Y$ the class of the observation, $\beta_{0k}$ the intercept and $\beta_{1k}, \dots, \beta_{pk}$ the regression coefficients.

: The softmax function is then used to convert the $K-1$ logit scores into probabilities for each class:

: $$
\mathbb{P}(Y = k_0 | X) = \frac{1}{1 + \sum_{j=1}^{K-1} e^{\beta_{0k} + \beta_k^TX }}
\hspace{5mm} \text{and} \hspace{5mm}
\mathbb{P}(Y = k | X) = \frac{e^{\beta_{0k} + \beta_k^TX }}{1 + \sum_{j=1}^{K-1} e^{\beta_{0k} + \beta_k^TX }}
$${#eq-softmax}

: Finally, the observation is classified in the class with the highest probability. To fit the model, ie to find the best values for the coefficients, we maximize the log likelihood function defined as:

: $$
l(\beta) = \sum_{i=1}^N log(\mathbb{P}(Y = y_i |x_i, \beta))
$${#eq-loglikelihood}

: Since the log-likelihood measures the probability of observing that exact data assuming a particular set of model parameters, maximizing this function allows us to find the best set of parameters.

: When we have a large set of variables, we can regularized the estimation by introducing penalty terms in the log-likelihood function. The two main types of penalization are the L1 regularization (also called a Lasso model) and the L2 regularization (also called a Ridge model). The Ridge regression prevents overfitting by adding the term $\lambda \sum_{k=1}^K \beta_k^2$ to the loss function, while the Lasso model selects features by adding the term $\lambda \sum_{k=1}^K |\beta_k|$ to the loss, giving a sparse solution with only some non-zero coordinates. The parameter $\lambda$ is called the regularization parameter.


Decision tree [@kuhn2013applied; [@breiman2017classification]]

: Decision trees consist of a nested sequence of if-then statements for the predictor classifying data. Observations are assigned to their class according to the variable values. We can see in the @fig-decisiontree an example of a tree with two splitting, leading to three leaf nodes. 

: ![Basic decision tree.](images/decision-tree.png){#fig-decisiontree width=250}

: The goal is to classify the observations into smaller and more homogeneous groups, forming rectangular areas within data points. Homogeneity is defined here as how pure the splitting nodes are , meaning that there is a higher proportion of one class in each node. We can use the Gini index to compute purity. In a scenario with $K$ classes, each having probability $p_1, \dots, p_K$, the Gini impurity for a certain node is defined as:

: $$
G(p) = \sum_{k=1}^K p_k \sum_{j \ne k}^K p_j = \sum_{k=1}^K p_k (1-p_k) = 1 - \sum_{k=1}^K (p_k)^2
$${#eq-gini}

: This index is minimal when one of the probabilities tends toward zero, indicating that the node is pure. To build the tree, the model builds splitting nodes by evaluating each splitting point (ie values taken by variables to split observations into groups) to find the one minimizing the Gini index, until a stopping criteria is met such as a maximum tree depth.

Bagged trees, Random forest and Boosted trees [@kuhn2013applied]

: A bagged trees model is an ensemble of $M$ decision trees trained in parallel. Each decision tree is built with a bootstrap sample of the original data, which is a sample of same size than the original data by selecting random observations with replacement. Then, each of these trees is used to generate a prediction for a new observation. Finally, the observation is classified in the class that has collected the greatest number of votes from all the trees.

: A random forest is a similar model, also using a number of decision trees. The model also trains a number of trees in parallel on bootstrap samples of the original data. The difference is that, at each split, a random subset of variables is selected to find the best predictors within this subset. The observations are then classified as usual. The prediction is done as seen before, with each tree voting for a class and selecting the majority to classify the observation.

: Finally, boosted trees train a number of decision trees sequentially to learn from the previous tree mistakes and put a higher weight on previously misclassified observations. The trees are also trained on bootstrapped samples, selecting a random subset of variables at each node. The final prediction is the weighted sum of the predictions from each tree with the weights determined by the performance of the trees.


$k$-Nearest Neighbors [@cover1967nearest]

: A $k$-Nearest Neighbors model finds the $k$ closest data points to a given input and makes prediction based on the majority class within its neighbors. This model is non-parametric as it does not make assumption about the underlying data.

: ![A point classified in the majority class within its $k = 3$ neighbors.](images/knn.png){#fig-knn width=300}

To calculate the distance between the input and its neighbors, the Minkowski distance can be used. It is defined as:

$$
d(x, y) = \left[ \sum_{i=1}^n (x_i-y_i)^p \right]^{1/p}
$${#eq-minkowski}

This distance is a generalization of multiple well-know distances as we find the Euclidean distance when $p = 2$ and the Manhattan distance when $p = 1$.


Neural networks [@varma2018]

: Neural Networks are deep learning models inspired by the human brain. They consist of interconnected nodes organized in layers which process the data by passing it from one layer to the next. The input data is transformed with non-linear functions, allowing the model to return a classification for each observation as output. The model contains two essential parameters: weights on each connection and biases on each node. It learns data pattern by adjusting these parameters during forward and backward propagation for each observation to minimize a loss that represents the difference between real and predicted values. Most often, the loss used is the cross-entropy which is defined as followed for the observation $m$:

: $$
\square(m) = - \sum_{k=1}^K t_k(m) \text{log}(y_k(m))
$$ {#eq-crossentropy}

: with:

: - $t_k$ the ground truth.
: - $y_k$ the classification probability.

: These models are complex and rely on a efficient algorithm for the parameter update and an adapted activation function, as well as possible utilization of batch normalization, regularization or dropout to find a model with a good complexity for the data. Tuning is done to find the best hyper-parameters such as the number of layers and nodes or the learning rate.


# Tuning and comparing segmentation models

## Dealing with imbalanced data

By classifying specific times in a time-serie, we work with imbalanced data. Especially in the first strategy when we predict events, a small proportion of the data is in the target classes, and most of the data points are in the negative class. In this situation, the model learns adequate information about the majority class but doesn't have enough information on the minority classes. This implies bad predictions on the target classes because the model misses them.

A lot of sampling algorithms exist to address this issue such as random oversampling or random undersampling which add or remove observations from certain classes to create a balanced dataset. More complex methods exist, we can cite the popular SMOTE [@chawla2002] and ADASYN [@haibo2008] algorithms that create synthetic data by considering the classes of the nearest neighbors of random observations. Although these algorithms are very popular, they did not show good results for our data. We are making the hypothesis that our data is too imbalanced for these methods.

To tell the model to pay more attention to the patterns in the minority classes, an effective way is to put a larger weight on these classes compared to the negative one. More precisely, the weight given to each observation specifies how much this observation influences the model's estimation In order for the model to be biased in favor of the observations considered more important, in our case those belonging to the minority classes, the weights of the observations are integrated into the cost function. This makes it possible to regulate the cost of misclassification, in the sense that misclassifying more important observations will be more costly, encouraging the model to avoid this situation [@hashemi2018].

When the classes are not too imbalanced, such as in our second strategy of phase prediction, we can use the inverse class frequency weights where the weight on the class $k$ is the following:

$$
\omega_k = \frac{n}{n_{classes} \hspace{1mm} n_k}
$$ {#eq-inv-class-frequency}

with:

- $n$ the total number of observations.
- $n_{classes}$ the number of classes.
- $n_k$ the number of observations in the class $k$.

This formula gives balanced weights but we can also choose class weights manually and tune them as one of our model parameter.


## Performance metrics

In classification problems, the most widely used metric is accuracy which is defined as the number of correct predictions divided by the total number of predictions. In our second strategy where we predict phases, we have four classes that are not severely unbalanced and without a negative class, thus we can simply use accuracy to tune and evaluate our models.

In the case of severe imbalanced classes like in our first strategy where we directly predict events, we cannot use accuracy to correctly evaluate the model. Indeed, since the model will predict almost only the majority class which is the negative one, the predictions in this class will be good, implying that the accuracy will be high because most of the observations will be well-classified. This does not take into account the observations in the minority classes which are not predicted.

We need to use other metrics that are adapted for this situation. An important tool is the confusion matrix which indicate the number of predictions in each class compared to the real values of the observations.

In a binary classification, The matrix contains the values of the True Positives ($TP$), False Positives ($FP$), True Negatives ($TN$) and False Negatives ($FN$). Two of the metrics that can be calculated from this matrix are known as Sensitivity and Specificity. They are defined as follow:

$$
\text{Sensitivity} = \frac{TP}{TP+FN} 
$${#eq-sensitivity}

$$
\text{Specificity} = \frac{TN}{TN+FP}
$${#eq-specificity}

The Sensitivity measures how well the model has found all occurences of our positive class, meaning that a low sensitivity implies a lot of missing observations in this class. The Specificity measures how well the model predicts the negative class. In a class imbalance problem, the Sensitivity is essential to know if the model achieved to predict correctly our minority class represented as the positive class.

In a multiclass classification, we can extend these metrics to use them with a different number of positive classes. In our case, we have four classes containing less observations and having to be correctly predicted, while the other class is the majority class. 

![Confusion matrix for five classes inclusing one negative one.](images/confusion-matrix.png){#fig-confusion-matrix width=300}

We define a confusion matrix with multiple positive classes and one negative class (see @fig-confusion-matrix). The element $c_{i,j}$ is the number of observations from the class $i$ that has been predicted in the class $j$. Therefore, $c_{i,.}$ represents the number of observations from the class $i$, while $c_{.,j}$ represents the number of observations that the model predicted to be in the class $j$. We now define the extension of Sensibility and Specificity with these notations, that can be called Macro-Average Sensibility and Macro-Average Specificity [@mortaz2020]. The formulas are generalized for a total of $K$ classes containing one negative class.

$$
\text{MA Sensitivity} = \frac{1}{K-1} \sum_{i=1}^{K-1} \frac{c_{i,i}}{c_{i,.}}
$${#eq-masensitivity}

$$
\text{MA Specificity} = \frac{1}{K} \sum_{i=1}^K \frac{\sum_{j \ne k \ne i} c_{j,k} + \sum_{j \ne i} c_{j,j}}{\sum_{j \ne i} c_{.,j}}
$${#eq-maspecificity}

These metrics can be used to evaluate our model to know if the classes of interest are correctly predicted.

To tune a model, we need a metric that combines the information from the previously defined MA Sensitivity and MA Specificity. The weighted Youden index [@li2013weighted] has this function, allowing the user to put a different weight on each metric. We generalize this index with the Macro-Average versions of the metrics:

$$
J_w = 2(w \times \text{MA Sensitivity} + (1-w) \times \text{MA Specificity}) -1
$${#eq-youden}

with $w \in [0,1]$ representing the weight put on the MA Sensitivity.

In our case, we chose to put a weight of $0.7$ on the MA Sensitivity to make sure not to forget observations in the minority classes.


## Data splitting and Tuning strategy

To correctly tune and evaluate the model, data is firstly split in two sets: the training set representing 80% of observations and the test set representing 20% of observations. The key is to never use the test set during training or tuning to then evaluate the model on data never seen before.

In certain models such as neural networks, the training set is divided furthermore into a train and a validation set, containing 80% and 20% of the training set respectively. In this case, the model computes the predictions on the train set and the loss over the validation set to determine the model parameters.

For tuning, the training set is used to create five folds which are random partitions of the training set to get five equal sized subsamples. In each fold, data is then split into a train and an evaluation set, containing 90% and 10% of the data respectively (see @fig-datasplitting). This ensures that the tuning result is relevant since since its outcome is the average of the scores obtained on the five folds.

![Data splitting.](images/data-splitting.png){#fig-datasplitting width=300}

To tune hyper-parameters, automated tuning was used by grid search which is an exhaustive search in the hyper-parameter space. For each of our parameter, we set a grid of values to be tested. Every combination of the values is then evaluated to get the best possible combination of hyper-parameter values for a certain metric. As mentionned before, we used the accuracy for the phase prediction and the weighted Youden index for the event prediction


## Results

### Tuning of hyper-parameters

### Selecting a model

### Performances on the test set

# Discussion and conclusions





# References {.unnumbered}

::: {#refs}
:::
