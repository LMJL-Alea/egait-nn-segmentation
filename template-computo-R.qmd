\newpage
\tableofcontents
\newpage

# Introduction

## Gait analysis

Study of the human gait has been shown to be important in various health applications, such as study of general health in elderly populations. It has been shown that gait parameters including walking speed or gait variability are very good indicators of their general health, whether it is their physical health or their cognition [@beauchet2016poor; @annweiler2009risk]. 

To monitor gait, various methods such as motion capture systems or pressure mats are widely used due to their reliability but they require patients to travel to a health center owning a device. Furthermore, the measures are made in front of medical workers leading to an observational bias in the patient's performance. To address these issues, wearable sensors are increasingly used for gait analysis as they are smaller, cheaper and more convenient than more traditional methods as they can be used outside of medical structures.

Gait analysis is based on the cyclical phenomenon of walking as it can be decomposed as a number of strides, each composed of two steps. A stride is more formally called a gait cycle and is a precise biomechanical concept which is the elementary element of gait analysis. We define a gait cycle as the set of movements performed between two consecutive contacts of the heel of the same foot with the ground. The goal of our method is to detect key events happening during gait cycles both to segment the gait as strides and to further study the gait by computing spatio-temporal parameters based on the events.

More precisely, the key events are the moments where a foot touches or leaves the ground. They are called *Heel Strike* and *Toe Off* respectively and they can be used to segment the gait cycle in phases. The stance phase corresponds to the moments when the foot is in contact with the ground and can be found between the *Right Heel Strike* and *Left Toe Off* events when a cycle starts with the right foot. The stance phase lasts generally 60% of the entire stride and is followed by the swing phase when the right foot is not anymore in contact with the ground (see @fig-walking-cycle).

![Events and phases of a typical gait cycle. [^jacquelin-perry]](images/walking-cycles.png){#fig-walking-cycle width=400}

[^jacquelin-perry]: Figure annotated from Jaquelin Perry's one on [Wikipedia](https://commons.wikimedia.org/wiki/File:GaitCycle_by_JaquelinPerry.jpg).

In healthy gaits, a gait cycle usually lasts one second and the time difference between two events can be as little as 100 ms. The events require a precise detection for an accurate analysis of gait parameters.


## State of the art

In the past, several methods for gait cycle segmentation and event detection have been studied based on signals recorded from wearable sensors placed on different parts of the body. We try in this part to give an overview of proposed methods by grouping them into categories, with variations related to the position of sensors on the body, the type of sensor measurement system used, and the characteristics of the signal.

Search for feature points: Peak Detection and zero-crossing

: Peak detection algorithms exploit the semi-periodic properties of the gait cycle, assuming that specific events during a gait cycle typically match the maxima or minima of a recorded signal. Several algorithms based on this method have been developed. Spatio-temporal gait parameters have been determined from the acceleration signal of a sensor by using peak detection and zero-crossing methods to identify stride cycles [@zijlstra2003assessment]. Some peak detection algorithms have been developed to work on any kind of signal [@jiang2017robust] or are specifically implemented to work to detect gait events on a signal, often Heel-Strike on acceleration data [@lueken2019peak]. Most methods detect Heel-Strike and Toe-off events by identifying minima, maxima or zero-crossing of acceleration time-series [@gonzalez2010real;@mariani2013quantitative;@panebianco2018analysis]. These events can be called Initial Contact and Terminal Contact and have also been identified on the foot inclination angle in the sagittal plane [@nazarahari2022foot].

Analysis of dynamic properties: Wavelet Transform

: To detect gait events in recorded signals, another used method is wavelet transform because it allows detection of a specified frequency at a specified time. This method has been used to detect *Heel Strike* and *Toe Off* events on an angular velocity signal by using multi-resolution wavelet decomposition [@aminian2002spatio]. The signal is decomposed into wavelet packages, using high-scale and low-scale filters. In @mccamley2012enhanced, the signal of the vertical acceleration is smoothed by integrating and then differentiating using a Gaussian Continuous Wavelet Transform (CTW). Events are found on minima and maxima of the differentiated signal. Furthermore, a method called Sparsity-assisted wavelet denoising (SWAD) has been developed to segment the signal in three events: mid-stance, toe-off and heel-strike [@prateek2019gait]. It uses a combination of linear time invariant filters, wavelets and sparsity based methods to extract a coefficient vector of Discrete Wavelet Transform (DTW). That generates a sparse template of moving segments of gyroscope measurements.

Pattern-matching: Dynamic Time Warping (DTW)

: To segment signals, another method is pattern-matching. Dynamic Time Warping is widely used for this matter. It allows identification of patterns with different length and it matches signals non-linearly. Thus it is commonly used to evaluate the similarity between time-series. This method has been used on an acceleration signal to identify gait cycles [@ghersi2020gait]. In another study, multi-dimensional subsequence DTW (msDTW) was used to segment strides using information from different axes of an accelerometer and a gyroscope at the same time [@barth2015stride].

Hidden Markov Models

: Hidden Markov Models (HMMs) are machine learning models that can be used to simultaneously segment and classify data. In gait analysis, hidden states of the HMM are viewed as activity classes or gait phases. Continuous HMM with Gaussian mixture model are often used to model the outputs. Each cycle can be modeled with a circular left to right HMM, included in a more global HMM model to classify walking activities [@panahandeh2013continuous]. Other algorithms have the goal of detecting four gait events which are Heel-Strike, Flat-Foot, Heel-Off and Toe-Off. They use a four state left to right HMM with observations following a multivariate Gaussian distributions [@mannini2011hidden;@garcia2022adaptive]. Hierarchical HMMs have shown to perform better than Peak detection and Dynamic Time Warping algorithms on walking data [@haji2018segmentation].

Other Machine Learning methods

: Many studies on gait analysis use wearable sensors and machine learning. A study showed that most used methods for this purpose are SVM and CNN, and are largely used to detect diseases or recognize activities when analyzing gait data [@saboor2020latest]. SVM divides a set of objects into classes with widest possible margin at the hyperplane boundaries. Neural Networks use interconnected nodes organized in layers to process information and learn patterns through back-propagation and make predictions using activation functions.

: Studies show good results when using machine learning to classify gait of patients having trouble walking, for instance patients with Parkinson Disease [@tahir2012parkinson;@wahid2015classification], or to extract gait parameters [@rampp2014inertial;@hannink2016sensor]. These studies mostly use Artificial Neural Networks but do not address the problem of the segmentation of the signal recorded by the sensors. Similarly, in [@farah2019design], a logistic regression tree is used to classify gait phases in a stride but not to directly segment the signal into strides.

: For the segmentation matter, Recurrent Neural Networks have shown good results for identifying Heel-Strike and Toe-off events with pressure sensors, accelerations and Euler angles [@prado2019gait]. Another study compared unsupervised machine learning (k-means) with supervised one (SVM and CNN), showing that CNNs were performing the best to predict stance and swing phases [@potluri2019machine]. Lastly, in another study, 24 time-series from three sensors’ accelerometers and gyroscopes were used as input into a 6 layers CNN to estimate the likelihood of the corresponding input sample, given a specific gait event [@gadaleta2019deep]. For instance, the signal can be segmented with the initial contact of the right foot if this gait event is chosen in the model. This method was shown to perform better than a Wavelet Transform algorithm.

We did not find in the literature a method based on hip rotations data represented as unit quaternion time-series which is the particularity of the data type used in our work. Furthermore, most methods presented either segment strides during a walking session or detect events inside already segmented strides. We propose here a method segmenting the gait cycles and detecting key gait events at the same time with a unique model.

The paper is structured as follows. The second section focuses on the data type used in the study, describing data acquisition, the raw data properties and the creation of the feature space. The third section describes the strategies and methods used to implement models in order to take into account the data specificity as well as accurately tuning and evaluating models. The fourth section presents the results leading to the choice and evaluation of a final model. We complete our work with a discussion and conclusion on the study. All codes were developed in R [@Rlanguage] using the {tidymodels} framework [@tidymodels].


# Material

This section details the data employed in this study. As unit quaternion time-series are used to represent rotations, we start by defining them and linking them to our goal.

## Unit quaternions

Unit quaternions can be used to represent the 3D rotation of an object over time [@hamilton1844;@voight2021quaternion]. This representation has several advantages, as it is a rather compressed representation containing only four values and avoiding the gimbal lock problem presents in other representations. Unit quaternions were therefore chosen as the data type returned by the sensor.

Quaternions are four-dimensional vectors denoted as $\mathbf{q} = \left( q_w, q_x, q_y, q_z \right)$, but can also be viewed as hypercomplex numbers of rank 4. Unit quaternions have a norm of 1 and can encode a 3D rotation with a rotation angle $\theta \in [0, 2\pi]$ and a rotation axis $\mathbf{u} = (u_x, u_y, u_z) \in S^2$, where $S^2$ is the 2-sphere, using the following formula:

$$
\mathbf{q} = \left( q_w, q_x, q_y, q_z \right)
= \text{cos}\left(\frac{\theta}{2}\right) + \left(u_x i + u_y j + u_z k \right) \text{sin}\left(\frac{\theta}{2}\right)
$$ {#eq-quaternions}

with :

- $i$, $j$, and $k$ generalizing the imaginary number $i$, as $i^2=j^2=k^2=ijk=-1$.
- $||\mathbf{q}|| = \sqrt{\mathbf{qq}^t} = 1$.

The set of unit quaternions, denoted $\mathbb{H}_u$, possesses interesting properties. The quaternions $\mathbf{q}$ and $-\mathbf{q}$ represent the same rotation. The group is equipped with an identity element $\mathbf{q}^{(0)} = (1,0,0,0)$ which corresponds to the identity rotation, such that $\mathbf{q} \mathbf{q}^{(0)} = \mathbf{q}^{(0)}\mathbf{q} = \mathbf{q}$.

It is possible to use the geodesic distance $d_g$ between two quaternions $\mathbf{q}_1$ and $\mathbf{q}_2$ to define a metric space $(\mathbb{H}_u, d_g)$, with:

$$
d_g (\mathbf{q}_1, \mathbf{q}_2) = ||\text{ln}(\mathbf{q}_1^{-1} \mathbf{q}_2)|| = \text{arccos}\left(2(\mathbf{q}_1 . \mathbf{q}_2)^2 -1\right)
$$ {#eq-dist-geodesique}

with the log-quaternion defined as $\text{ln}(\mathbf{q}) =  \text{ln}\left(\text{exp}\left({\mathbf{u}\dfrac{\theta}{2}}\right)\right)$.

In this application, the sensor orientation is the rotation between the reference frame of origin, here the Earth's reference frame $R_f=(f_1, f_2, f_3)$ and its own reference frame $R_s=(s_1, s_2, s_3)$ formed by the accelerometer, gyroscope, and magnetometer (see @fig-sensor-axis).

![Sensor reference and axis. [@drouin2023semi]](images/sensor-axis.png){#fig-sensor-axis width=200}

A quaternion time-serie is a set of unit quaternions following a temporal grid $t_1, \dots, t_p$, noted as $\mathbf{Q}_i = (\mathbf{q}_{1i}, \dots, \mathbf{q}_{pi})$. It represents the consecutive 3D hip rotation over time, as four-component vectors.


## Data acquisition

A wearable sensor was used to record the hip rotation. It contains an accelerometer, a gyroscope and a magnetometer. Subjects were wearing the sensor on their right hip (see @fig-sensor-position). The frequency of the sensor is 100 Hz so data is acquired every 10 ms. With this device, the data acquired is in the form of unit quaternion time-series as presented previously.

![Sensor positionned on the right hip.](images/sensor-position.png){#fig-sensor-position width=150}

During acquisitions, subjects were walking on the GAITRite© mat, a gold standard in gait analysis [@Menz2004] while wearing the wearable sensor on their right hip. It implies that subjects were walking approximately nine meters on a straight line. The GAITRite© device gives various information thanks to pressure sensors contained in the mat such as the times where the subject feet touche and leave the ground at each step, meaning that these times are used to know when the gait events actually happened. This data is then used label the quaternion time-series to train the model. To use the two devices simultaneously, they were started at the same time by the same person with their two indexes, allowing a good synchronization between devices.

Six subjects have been included in this study (three men and three women) walking at different speeds to have a variety of gait data (see @tbl-subjects). We have in total 174 walking sessions acquired between June and September 2024.

::: {#tbl-subjects tbl-pos="H"}

```{r}
data.frame(
  id = c("MBA", "MBO", "MSI", "MTR", "NNE", "TDE"),
  gender = c("M", "F", "F", "M", "F", "M"),
  age = c("50-60", "20-30", "20-30", "20-30", "20-30", "50-60"),
  slow = c(60, 73, 67, 77, 57, 61),
  intermediate = c(116, 122, 115, NA, 116, NA),
  preferential = c(145, 145, 148, 132, 147, 120),
  fast = c(199, 188, 179, 185, 190, 193)
) |> 
  gt::gt() |> 
  gt::tab_spanner(
    label = "Walking speed (cm/s)",
    columns = c(slow, intermediate, preferential, fast)
  ) |> 
  gt::cols_label(
    id = "ID",
    gender = "Gender",
    age = "Age range (years)",
    slow = "Slow",
    intermediate = "Intermediate",
    preferential = "Preferential",
    fast = "Fast"
  ) |> 
  gt::sub_missing() |> 
  gt::opt_stylize(style = 6, color = 'gray') |> 
  gt::cols_align(align = "center") |> 
  gt::tab_style(
    style = "vertical-align:top", 
    locations = gt::cells_column_labels()
  )
```

Summary of subjects and walking speeds used to train the model.

:::

## Data presentation

Sensor data

: As mentioned before, the wearable sensor returns unit quaternion time-series representing the rotation of the hip over time, allowing visualization of each coordinate time-serie (see @fig-timeserie) at each recorder time. The four coordinates comes from the definition of quaternions as defined in @eq-quaternions. It is important to note that on this figure, the gait cycles are clearly apparent and consistent over time, as it represents a healthy gait, which would not necessarily be the case for subjects having gait disorders.

![Data returned by the wearable sensor as a four-coordinate unit quaternion time-serie.](images/time-serie.png){#fig-timeserie width=350}

Sensor data preprocessing 

: A centring step is applied on the quaternion time-series to center them around a mean. Supposing we have $n$ time-series $\mathbf{Q}_1, \dots, \mathbf{Q}_n$ on the same time grid $t_1, \dots, t_p$, we can write $\mathbf{Q}_i(t_j) = \mathbf{q}_{ij} \in \mathbb{H}_u$, with $i \in [\![ 1, n ]\!]$ and $j \in [\![ 1, p ]\!]$. We use the Fréchet mean associated to the geodesic distance $d_g$ (see @eq-dist-geodesique) to compute the mean of each quaternion $\mathbf{q}_{1j}, \dots, \mathbf{q}_{nj}$ for each time $t_j$. 

: $$
\mathbf{q}_j^{(m)} = \mathbf{Q}^{(m)} (t_j) = \underset{\mathbf{q} \in \mathbb{H}_u}{\mathrm{argmin}} \sum_{i=1}^n d_g^2(\mathbf{q}_{ij}, \mathbf{q}), \hspace{5mm} j \in [\![ 1, p ]\!]
$$ {#eq-mean-qts}

: The centered time-series $\mathbf{Q}_1^{(c)}, \dots, \mathbf{Q}_n^{(c)}$ can then be computed.

: $$
\mathbf{q}_{ij}^{(c)} = \mathbf{Q}_i^{(c)} (t_j) = \left( \mathbf{q}_j^{(m)} \right)^{-1} \mathbf{q}_{ij}, \hspace{5mm} j \in [\![ 1, p ]\!],\hspace{2mm}  i \in [\![ 1, n ]\!]
$$ {#eq-centring-qts}

: The other pre-processing step is to switch to a functional representation using cubic splines to be able to compute derivatives [@ramsay2005].


Pressure mat data

: The GAITRite© mat measures the feet position on the mat with its sensors while the subject walks on it. It returns directly spatio-temporal parameters such as stride duration, stride length or walking speed (see @tbl-gaitrite-params in the Appendix for a list of all parameters). It also returns the time of each event happening during a gait cycle such as the time where a foot touches or leaves the ground. These are the times we use to label our data to predict these events. Since the two devices were triggered simultaneously, the same time range is used to associate each time on the IMU time-series with a gait event recorder by the mat. We use the pressure mat as a gold standard to label the observations between the different classes and train the model on this labeled data.


## Feature space

The feature space is defined by a set of variables that captures the time-varying characteristics of hip rotation. It forms the data used to build machine learning models that will learn its pattern to detect the events associated to each time.

Angular velocity and acceleration

: Supposing we can compute first and second derivatives of a quaternion time-series over time, we can compute angular velocity and acceleration [@narayan2017]. The angular velocity $\pmb{\Omega}$ is a vector which has for direction the axis of rotation and for quantity the angular velocity.

: $$
\pmb{\Omega} = 2 \mathbf{q}^{-1} \dot{\mathbf{q}} \hspace{3mm} \text{with} \hspace{3mm} \dot{\mathbf{q}} = \frac{d \mathbf{q}}{dt} = \frac{1}{2} \mathbf{q} \hspace{1mm} \pmb{\Omega}
$$ {#eq-angular-vel}

: Similarly, the angular acceleration $\dot{\pmb{\Omega}}$ is the angular velocity derivative.

: $$
\dot{\pmb{\Omega}} = 2 \left( \mathbf{q}^{-1} \ddot{\mathbf{q}} - (\mathbf{q}^{-1}\dot{\mathbf{q}})^2 \right) \hspace{3mm} \text{with} \hspace{3mm} \ddot{\mathbf{q}} = \frac{d^2 \mathbf{q}}{dt^2} = \frac{1}{2} \left( \dot{\mathbf{q}} \hspace{1mm}  \pmb{\Omega} + \mathbf{q} \hspace{1mm}  \dot{\pmb{\Omega}} \right)
$$ {#eq-angular-acc}

Euler angles

: The angles named Roll, Pitch and Yaw represent rotations around the three principal axis. Their computation is done using the quaternion time-series, with the following rotation matrix to go from the quaternion $\mathbf{q} = (q_w, q_x, q_y, q_z)$ to the angles.

$$
\begin{bmatrix}
\text{Roll} \\
\text{Pitch} \\
\text{Yaw}
\end{bmatrix}
= 
\begin{bmatrix}
\text{atan2} \left(2(q_w q_x + q_y q_z), 1-2(q_x^2 + q_y^2)  \right) \\
\text{asin} \left(2(q_w q_y - q_x q_z) \right) \\
\text{atan2} \left(2(q_w q_z + q_x q_y), 1-2(q_y^2 + q_z^2)  \right)
\end{bmatrix}
$$ {#eq-rpy}

with $\text{atan2}(y, x)$ returning the angle $\theta$ (in radians) between the positive $x$-axis and the ray from the origin to the point $(x, y)$ in the Cartesian plane.


Walking speed

: One of our hypothesis is that the gait can differ depending if the subject walks slowly of faster. Thus this variable was also added to the feature space by getting it from the GAITRite© mat output.

Feature Space Hyper-parameters

: The feature space depends on a number of hyper-parameters. The first one is a smoothness parameter called *spar* for the time-serie curves. Since derivation is used to compute some predictors, it is useful to smooth the curves so the values are stable and not dominated by noise. The *spar* parameter takes its values in the interval $]0,1]$ and is used to fit a cubic smoothing spline to the data. The second hyper-parameter is the *lag* parameter. To keep information from the past as we predict at a certain time, we add on the observation $t_j$ the predictor values from the times $t_{j-1}, \dots, t_{j-lag}$. The feature space length depends on the *spar* parameter as it contains $10 + 9 \times lag$ variables. Finally, a last hyper-parameter is used when we label the observations with the event times from the gold standard. It is called *k* and controls the number of points that we label as part of the event. Since it is not used in both compared strategies, it is explained in the next section while we describe the two strategies.


# Methods

## Classification strategies

Gait event detection is performed by evaluating and comparing two strategies to classify the observations.

Strategy E: Predicting gait Events

: The strategy E is to directly predict the gait events happening during the walk. We classify the observations in five classes depending if the time corresponds to an event or not:

: - *Right Heel Strike*
- *Left Toe Off*
- *Left Heel Strike*
- *Right Toe Off*
- *None* (all other times not corresponding to a certain event)

: This strategy is the most direct one as we obtain the event times with the gold standard. However, this classification raises an issue of severe class imbalance with the *None* class widely over-represented. Indeed, the class that represents all the times that do not belong to an event is clearly larger than the four other ones. It is clearly represented on the @fig-event-timeserie where we can see the event times from the GAITRite© mat overlaid on the time-serie recorded by the IMU. Each colored point represents a different event and all other times belong to the *None* class.

: ![Time of events represented on a unit quaternion time-serie.](images/events_on_time_serie_MSI_N_R2.png){#fig-event-timeserie width=350}

: When predicting events, we choose to label a number of points $k$ as corresponding to the event around the precise event time given by the gold standard. This allows to take into account some uncertainty, as the time range between two points is only 10 ms. For instance, if we have $k=1$, we label 3 observations as part of the event rather than just one. In this case we consider that the event happens in a window of 20 ms instead of just at a precise time. This strategy also reduces somewhat the imbalance between the class proportions.

\newpage

Strategy P: Predicting sub-Phases

: The strategy P consists of predicting sub-phases in the signal that are characterized by the gait events. We label each part in the signal between two events as a sub-phase, leading to four classes for our observations:

: - *Pre-Stance* phase: between *Right Heel Strike* and *Left Toe Off*
- *Stance* phase: between *Left Toe Off* and *Left Heel Strike*
- *Pre-Swing* phase: between *Left Heel Strike* and *Right Toe Off*
- *Swing* phase: between *Right Toe Off* and *Right Heel Strike*

: To name these classes, we based ourselves on a typical cycle starting with the right foot, starting with a stance phase while the right foot is on the ground followed by a swing phase. This classification allows us to have four classes that are less imbalanced, without a class that does not contain events like the *None* class in the first strategy. The downside is that we loose some precision as for the event times and that we need to find the events from the predicted phases after the model predictions.

: ![Example of phases represented on a unit quaternion time-serie.](images/phases_on_time_serie_MSI_N_R2.png){#fig-phases-timeserie width=350 fig-pos="H"}

: We represent the sub-phases on the IMU time-serie in the @fig-phases-timeserie where each sub-phase is represented by a different color. It shows that some sub-phases are larger than others as it is clear on this example that the sub-phases *Pre-Stance* and *Pre-Swing* are shorter than the *Stance* and *Swing* ones.


## Supervised classification models

Now that we defined our feature space predictors and the different classes used to classify the data, we can build supervised classification models with them. We define in this part the machine learning models that we compare for both strategies.

Multinomial Regression [@hosmer2013applied]

: A multinomial regression is built by fitting a set of independent binary logistic regressions. We want to classify our observations in $K$ classes. We choose one category to be the baseline and name it $k_0$. We build $K-1$ logit equations modeling the log-odds of a class $k$ versus the class $k_0$. The logit equation for a class $k$ is:

: $$
log\left( \frac{\mathbb{P}(Y = k | X)}{\mathbb{P}(Y = k_0 | X)} \right) = \beta_{0k} + \beta_k^TX
$${#eq-logit}

: with $Y$ the class of the observation, $X$ the predictors, $\beta_{0k}$ the intercept and $\beta_{1k}, \dots, \beta_{pk}$ the regression coefficients.

: The softmax function is then used to convert the $K-1$ logit scores into probabilities for each class. Finally, the observation is classified in the class with the highest probability. To fit the model, ie to find the best values for the coefficients, we maximize the log likelihood function.

: When we have a large set of variables, we can regularized the estimation by introducing penalty terms in the log-likelihood function [@hastie2015statistical]. The two main types of penalization are the L1 regularization (also called a Lasso model) and the L2 regularization (also called a Ridge model). The Ridge regression prevents overfitting by adding the term $\lambda \sum_{k=1}^K \beta_k^2$ to the loss function, while the Lasso model selects features by adding the term $\lambda \sum_{k=1}^K |\beta_k|$ to the loss, giving a sparse solution with only some non-zero coordinates. The parameter $\lambda$ is called the regularization parameter.


Decision tree [@kuhn2013applied; @breiman2017classification]

: Decision trees consist of a nested sequence of if-then statements for the predictor classifying data. Observations are assigned to their class according to the variable values. 
<!-- We can see in the @fig-decisiontree an example of a tree with two splitting, leading to three leaf nodes.  -->

<!-- : ![Basic decision tree.](images/example_tree.png){#fig-decisiontree width=250} -->

: The goal is to classify the observations into smaller and more homogeneous groups, forming rectangular areas within data points. Homogeneity is defined here as how pure the splitting nodes are , meaning that there is a higher proportion of one class in each node. We can use the Gini index to compute purity. In a scenario with $K$ classes, each having probability $p_1, \dots, p_K$, the Gini impurity for a certain node is defined as:

: $$
G(p) = \sum_{k=1}^K p_k \sum_{j \ne k}^K p_j = \sum_{k=1}^K p_k (1-p_k) = 1 - \sum_{k=1}^K (p_k)^2
$${#eq-gini}

: This index is minimal when one of the probabilities tends toward zero, indicating that the node is pure. To build the tree, the model builds splitting nodes by evaluating each splitting point (ie values taken by variables to split observations into groups) to find the one minimizing the Gini index, until a stopping criteria is met such as a maximum tree depth.

Bagged trees, Random forest and Boosted trees [@kuhn2013applied]

: A bagged trees model is an ensemble of $M$ decision trees trained in parallel. Each decision tree is built with a bootstrap sample of the original data, which is a sample of same size than the original data by selecting random observations with replacement. Then, each of these trees is used to generate a prediction for a new observation. Finally, the observation is classified in the class that has collected the greatest number of votes from all the trees.

: A random forest is a similar model, also using a number of decision trees. The model also trains a number of trees in parallel on bootstrap samples of the original data. The difference is that, at each split, a random subset of variables is selected to find the best predictors within this subset. The observations are then classified as usual. The prediction is done as seen before, with each tree voting for a class and selecting the majority to classify the observation.

: Finally, boosted trees train a number of decision trees sequentially to learn from the previous tree mistakes and put a higher weight on previously misclassified observations. The trees are also trained on bootstrapped samples, selecting a random subset of variables at each node. The final prediction is the weighted sum of the predictions from each tree with the weights determined by the performance of the trees.


$k$-Nearest Neighbors [@cover1967nearest]

: A $k$-Nearest Neighbors model finds the $k$ closest data points to a given input and makes prediction based on the majority class within its neighbors. This model is non-parametric as it does not make assumption about the underlying data.

<!-- : ![A point classified in the majority class within its $k = 3$ neighbors.](images/example_3nn.png){#fig-knn width=300} -->

To calculate the distance between the input and its neighbors, the Minkowski distance can be used. It is defined as:

$$
d(x, y) = \left[ \sum_{i=1}^n (x_i-y_i)^p \right]^{1/p}
$${#eq-minkowski}

This distance is a generalization of multiple well-know distances as we find the Euclidean distance when $p = 2$ and the Manhattan distance when $p = 1$.


Neural networks [@varma2018]

: Neural Networks are deep learning models inspired by the human brain. They consist of interconnected nodes organized in layers which process the data by passing it from one layer to the next. The model contains two essential parameters: weights on each connection and biases on each node. The input data is given to the input layer and then passes through the hidden layers. At each layer, a pre-activation is computed with the weights and biases and is transformed as an activation with non-linear functions (usually the function *ReLU*). Finally, the result goes into the output layer which uses an output function (the *softmax* function for more than two classes) to return the predicted class for each observation. It learns data pattern by adjusting the parameters during forward and backward propagation for each observation to minimize a loss that represents the difference between real and predicted values. Most often, the loss used is the cross-entropy which is defined as followed for the observation $m$:

: $$
\square(m) = - \sum_{k=1}^K t_k(m) \text{log}(y_k(m))
$$ {#eq-crossentropy}

: with:

: - $t_k$ the ground truth.
: - $y_k$ the classification probability.

: These models are complex and rely on a efficient algorithm for the parameter update and an adapted activation function, as well as possible utilization of batch normalization, regularization or dropout to find a model with a good complexity for the data. Tuning is done to find the best hyper-parameters such as the number of layers and nodes or the learning rate.

## Dealing with imbalanced data

By classifying specific times in a time-serie, we work with imbalanced data. Especially in the strategy E when we predict events, a small proportion of the data is in the target classes, and most of the data points are in the negative class. In this situation, the model learns adequate information about the majority class but doesn't have enough information on the minority classes. This implies bad predictions on the target classes because the model misses them.

A lot of sampling algorithms exist to address this issue such as random oversampling or random undersampling which add or remove observations from certain classes to create a balanced dataset. More complex methods exist, we can cite the popular SMOTE [@chawla2002] and ADASYN [@haibo2008] algorithms that create synthetic data by considering the classes of the nearest neighbors of random observations. Although these algorithms are very popular, they did not show good results for our data. We are making the hypothesis that our data is too imbalanced for these methods.

To tell the model to pay more attention to the patterns in the minority classes, an effective way is to put a larger weight on these classes compared to the negative one. More precisely, the weight given to each observation specifies how much this observation influences the model's estimation In order for the model to be biased in favor of the observations considered more important, in our case those belonging to the minority classes, the weights of the observations are integrated into the cost function. This makes it possible to regulate the cost of misclassification, in the sense that misclassifying more important observations will be more costly, encouraging the model to avoid this situation [@hashemi2018].

When the classes are not too imbalanced, such as in our second strategy of phase prediction, we can use the inverse class frequency weights where the weight on the class $k$ is the following:

$$
\omega_k = \frac{n}{n_{classes} \hspace{1mm} n_k}
$$ {#eq-inv-class-frequency}

with:

- $n$ the total number of observations.
- $n_{classes}$ the number of classes.
- $n_k$ the number of observations in the class $k$.

This formula gives balanced weights but we can also choose class weights manually and tune them as one of our model parameter.


## Performance metrics

To tune and evaluate the models, we need to use metrics adapted to the data. Here, depending on the strategy used, we can't use the same metrics as the labeled classes are very different.

First of all, for the strategy P predicting sub-phases, we have four classes that are not severely unbalanced and all having the same importance. In this case, we can simply use the accuracy metric to tune and evaluate our models. It is the most widely used metric in machine learning and is defined as the number of correct predictions divided by the total number of predictions

For the strategy E where we directly predict events, we are in a case of severe class imbalance so we can't use accuracy to correctly evaluate the model. Indeed, since the model will predict almost only the majority class which is the negative one, the predictions in this class will be good, implying that the accuracy will be high because most of the observations will be well-classified. This does not take into account the observations in the minority classes which are not predicted.

We need to use other metrics that are adapted to this situation. An important tool is the confusion matrix which indicate the number of predictions in each class compared to the real classes of the observations.

In a binary classification, The matrix contains the values of the True Positives ($TP$), False Positives ($FP$), True Negatives ($TN$) and False Negatives ($FN$). Two of the metrics that can be calculated from this matrix are known as Sensitivity and Specificity. The Sensitivity measures how well the model has found all occurrences of our positive class, meaning that a low sensitivity implies a lot of missing observations in this class. The Specificity measures how well the model predicts the negative class. In a class imbalance problem, the Sensitivity is essential to know if the model achieved to predict correctly our minority class represented as the positive class.

In a multiclass classification, we can extend these metrics to use them with a different number of positive classes. In our case, we have four classes containing less observations and having to be correctly predicted, while the other class is the majority class. 

We define a confusion matrix with multiple positive classes and one negative class (see @fig-confusion-matrix). The element $c_{i,j}$ is the number of observations from the class $i$ that has been predicted in the class $j$. Therefore, $c_{i,.}$ represents the number of observations from the class $i$, while $c_{.,j}$ represents the number of observations that the model predicted to be in the class $j$.

![Confusion matrix for five classes including the *None* one represented by the letter N and four classes of interest named P1 to P4.](images/confusion-matrix.png){#fig-confusion-matrix width=300 pos="H"}

We now define our extensions of Sensibility and Specificity with these notations, that can be called Macro-Average Sensibility and Macro-Average Specificity [@mortaz2020]. The formulas are generalized for a total of $K$ classes including one class not containing any relevant information, in our case the *None* class.

$$
\text{MA Sensitivity} = \frac{1}{K-1} \sum_{i=1}^{K-1} \frac{c_{i,i}}{c_{i,.}}
$${#eq-masensitivity}

$$
\text{MA Specificity} = \frac{1}{K} \sum_{i=1}^K \frac{\sum_{j \ne k \ne i} c_{j,k} + \sum_{j \ne i} c_{j,j}}{\sum_{j \ne i} c_{.,j}}
$${#eq-maspecificity}

These metrics can be used to evaluate our model to know if the classes of interest are correctly predicted.

To tune a model, we need a metric that combines the information from the previously defined MA Sensitivity and MA Specificity. The weighted Youden index [@li2013weighted] has this function, allowing the user to put a different weight on the Sensitivity and the Specificity. We generalize the weighted Youden index with the Macro-Average versions of the metrics and name it the Generalized Weighted Youden Index (GWYI):

$$
\text{GWYI} = 2(w \times \text{MA Sensitivity} + (1-w) \times \text{MA Specificity}) -1
$${#eq-youden}

with $w \in [0,1]$ representing the weight put on the MA Sensitivity.

The weight parameter controls the importance of each metric. The greater it is, the more weight we put on the MA Sensitivity. In our case, we chose to put a weight of $0.7$ on the MA Sensitivity because we wanted to make sure not to forget observations in the minority classes, even if it meant predicting too many of these observations.


## Data splitting and Tuning strategy

To correctly tune and evaluate the model, data is initially split in two sets: the training set representing 80% of observations and the test set representing 20% of observations. The key is to never use the test set during training or tuning to be able to evaluate the model on data never seen before. We chose to keep entire walking sessions in each set so that the model would never see any observation of the sessions contained in the test set during training. We had 139 sessions in the training set and 35 sessions in the test set.

In certain models such as neural networks, the training set is divided furthermore into a train and a validation set, containing 80% and 20% of the training set respectively. In this case, the model computes the predictions on the train set and the loss over the validation set to determine the model parameters.

For tuning, the training set is used to create five folds which are random partitions of the training set to get five equal sized sub-samples. In each fold, data is then split into a train and an evaluation set, containing 90% and 10% of the data respectively (see @fig-datasplitting). This ensures that the tuning result is relevant since since its outcome is the average of the scores obtained on the five folds.

![Data splitting.](images/data-splitting.png){#fig-datasplitting width=600}

To tune hyper-parameters, automated tuning was used by grid search which is an exhaustive search in the hyper-parameter space. For each of our parameter, we set a grid of values to be tested. Every combination of the values is then evaluated to get the best possible combination of hyper-parameter values for a certain metric. As mentioned before, we used the accuracy for the phase prediction and the GWYI for the event prediction.

Now that we defined all methods and tools necessary to build machine learning models on our data, we can present our results.

# Results

Various models are built and evaluated to find the best model for each strategy. Then, both strategies are compared to select the best one and have a resulting final model.

## Tuning of hyper-parameters

In order to choose the best model for each strategy, we tuned the hyper-parameter of each model for both strategies. Since we do not use the same metric to tune and evaluate models for each strategy, as we use the GWYI in the strategy E and accuracy in the strategy P, we cannot directly choose a strategy at this step. Therefore, we select one model per strategy and compare the two strategies later.

We choose a set of values to test for each hyper-parameter of the feature space tuning (see @tbl-grid-fs). We recall that the hyper-parameters *spar* and *n_lag* are used in both strategies but the hyper-parameter *k* is used only in the strategy E to label events.

::: {#tbl-grid-fs tbl-pos="H"}

```{r}
tibble::tibble(
  param = c("spar", "n_lag", "k"),
  description = c("Smoothing curve parameter", "Kept times in the past", "Number of labeled points"),
  strategy = c("Both", "Both", "Strategy E"),
  values = c("0.3, 0.4, 0.5, 0.6, 0.7", "1, 2, 3, 4, 5", "1, 2, 3")
) |>
  gt::gt() |> 
  gt::cols_label(
    param = "Parameter",
    strategy = "Strategy",
    description = "Description",
    values = "Tuning Grid"
  ) |> 
  gt::cols_align(
    align = "left",
    columns = values
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = param)
  ) |> 
  gt::tab_options(column_labels.background.color = "#616161")
```

Feature space parameters tuning grid values.

:::

For the strategy E, we also tune the weight classes used to address the class imbalance issue. For this matter, we fix the weight of the class *None* with the value 1, and we test different weights for the classes of interest (see @tbl-grid-class-weights). For the strategy P, as the class imbalance is less present, we only use the weights from the inverse class frequency method defined in @eq-inv-class-frequency.

::: {#tbl-grid-class-weights tbl-pos="H"}

```{r}
is_empty <- function(x) {
  return(x == "")
}

options(gt.html_tag_check = FALSE)
tibble::tibble(
  param = c("Weight on class *None*", "Weight on event classes"),
  values = c("1", "40, 60, 80")
) |> gt::gt() |> 
  gt::cols_label(
    param = "Parameter",
    values = "Tuning Grid"
  ) |> gt::tab_options(column_labels.background.color = "#616161") |> 
  gt::fmt_markdown(columns = param)
```

Strategy E: Class weight tuning grid values.

:::

Finally, we build tuning grid values for every model hyper-parameters which are common to both strategies (see @tbl-grid-model).

::: {#tbl-grid-model tbl-pos="H"}

```{r}
tibble::tibble(model = c(
  "Multinomial Regression", "Multinomial Regression",  
  "Decision Tree", "Decision Tree",
  "Bagged Trees", "Bagged Trees",
  "Random Forest", "Random Forest",
  "Boosted Trees", "Boosted Trees", "Boosted Trees", "Boosted Trees",
  "KNN", "KNN",
  "Neural Network", "Neural Network", "Neural Network"
),
param = c(
  "`penalty`", "`mixture`",
  "`tree_depth`", "`min_n`",
  "`tree_depth`", "`min_n`",
  "`trees`", "`min_n`",
  "`trees`", "`min_n`", "`tree_depth`", "`learn_rate`",
  "`neighbors`", "`dist_power`",
  "`hidden_units`", "`learn_rate`", "`dropout`"
),
description = c(
  "Regularization parameter", "Regularization type (Lasso, Ridge, Elastic Net)",
  "Tree depth", "Minimal number of observations before splitting",
  "Tree depth", "Minimal number of observations before splitting",
  "Number of trees", "Minimal number of observations before splitting",
  "Number of trees", "Minimal number of observations before splitting", "Tree depth", "Learning rate",
  "Number of nearest neighbors", "Minkowski distance order",
  "Hidden layers and units", "Learning rate", "Rate of randomly deactivated nodes"
),
values = c(
  "0.01, 0.001, 0.0001", "0, 0.25, 0.5, 0.75, 1",
  "1:15", "2:40",
  "1:15", "2:40",
  "100, 300, 500, 700, 900", "2:40",
  "400, 600, 800, 1000", "20, 40", "5, 8, 12", "0.01",
  "3, 5, 10, 20", "1, 2",
  "hidden layers: 2:5; nodes: 100, 500", "0.01, 0.001", "0.1, 0.3"
)) |> 
  dplyr::group_by(model) |> 
  dplyr::mutate(model = ifelse(dplyr::row_number() == 1, model, "")) |> 
  dplyr::ungroup() |> 
  gt::gt() |>
  gt::tab_style(
    style = list(
      gt::cell_borders(
        sides = c("top", "bottom"),
        weight = gt::px(0)
      )),
    locations = list(
      gt::cells_body(
        columns = model,
        rows = is_empty(model)
      )
    )
  ) |> gt::cols_label(
    model = "Model",
    param = "Parameter",
    description = "Description",
    values = "Tuning Grid"
  ) |> 
  gt::cols_width(
    model ~ "25%",
    param ~ "17%", 
    description ~ "38%",
    values ~ "25%"
  ) |>  
  gt::tab_options(column_labels.background.color = "#616161") |> 
  gt::fmt_markdown(columns = param)
```

Model hyper-parameter tuning grid values.

:::

For each strategy and each model, we get the optimal parameters thanks to tuning (see @tbl-tuning-res-events and @tbl-tuning-res-phases in the Appendix for the detail of chosen parameters for each model). We then evaluate each model on the test set which was never seen before to find the best model for each strategy (see @tbl-res).

::: {#tbl-res tbl-pos="H"}

```{r}
data.frame(
  model = c("Multinomial Regression", "Decision Tree", "Bagged Trees", "Random Forest", "Boosted Trees", "KNN", "Neural Network"),
  youden = c("67.6%", "60.0%", "70.3%", "70.5%", "83.0%", "24.0%", "88.7%"),
  accuracy = c("82.7%", "73.9%", "74.9%", "89.4%", "89.7%", "88.0%", "89.4%")
) |> 
  gt::gt() |> 
  gt::cols_label(
    model = "Model",
    youden = "Strategy E: GWYI",
    accuracy = "Strategy P: accuracy"
  ) |> 
  gt::tab_options(column_labels.background.color = "#616161") |> 
  gt::tab_style(
    style = gt::cell_fill(color = 'lightgreen'),
    locations = gt::cells_body(columns = c(youden), rows = 7)
  ) |>
  gt::tab_style(
    style = gt::cell_fill(color = 'lightgreen'),
    locations = gt::cells_body(columns = c(accuracy), rows = 5)
  ) |> gt::cols_width(
    everything() ~ px(200)
  )
```

Performance metrics on the test set for both strategies.

:::

We keep one model per strategy to maximize the performance metric evaluated on the test set. For the strategy E, we select the Neural Network model to maximize the GWYI. For the strategy P, we select the Boosted Trees model to maximize accuracy. We can note that for the strategy P, three models (the random forest, the boosted trees and the neural network) had close results between 89.4% and 89.7%. However, since all models had really quick computing times to make predictions on the test set, this was not a criteria to choose one model over another one, so we could simply choose the model giving the highest score.

Now that we selected a model for each strategy, the next step is to compare the two strategies to choose the best one, leading to a final model.

## Selecting a strategy

To choose a final model, we need to determine the best strategy. In order to do so, we compare the predictions made on the test set by the two previously selected models with the real events given by the gold standard to see which model gives closer results. We still use the test set to evaluate the strategies.

First, we shorten the time-series to retain the times where we want to detect points by keeping only the time range where the gold standard detected the four events perfectly. Indeed, at the start and the end of the session, this tool can miss contact points because the subject is not walking on the mat sensors. At the start of the session, we search for the first *Heel Strike* event which is followed by the three other events in the correct order. At the end of the session, we remove all points after the first missing event.

In both strategies, the model does not detect only one point corresponding to the event but a window of points containing the event (see @fig-preds-both-strategies). This means that we need to build a procedure to get precise estimated event times from the model predictions. In the strategy P, when predicting sub-phases, the event corresponding to the sub-phase is theoretically located at the start of the sub-phase but we see that it is not always the case with our model's predictions. Hence we use the same process for both strategies to choose one point either in the windows detected with the strategy E or in the sub-phases predicted with the strategy P.


::: {#fig-preds-both-strategies layout="[45, -10, 45]" fig-pos="H"}

![Strategy E: Predictions made by the neural network.](images/preds_events_and_real_events.png){#fig-preds-events width=270}

![Strategy P: Predictions made by the boosted trees.](images/preds_phases_and_real_events.png){#fig-preds-phases width=270}

Predictions from both strategies. The bigger and darker points represent the real time of events given by the GAITRite© gold standard.
:::

In each window or phase detected, the following process is applied to keep one point representing the event. We first need to identify each of them correctly, using the two following parameter:

- A threshold to firstly select only the observations with a high probability of being in the class: $p > 0.4$.
- A time to know when to distinguish two predicted windows or phases: if the first point of the window $i$ is at more than $t = 40 ms$ from the last point of the window $i-1$ we consider the windows distinct. It enables us to define unique windows with points that are not necessarily consecutive when only one event should be detected in this time range.

Then, in each of these clearly separated windows or phases, we keep the point having the highest probability of being in this class according to the model. This point is the time of the estimated event. The last step is to compare these estimated events with the ones given by the gold standard to see which of the strategy gives better result and select a final strategy.

The first point of comparison is the number of each event detected during the walking sessions. For both strategies, the model predicts the exact number of events for every classes in each session. It means that both models seem to perform well but it does not allow us to tell them apart. The second item that can be compared is the time at which the events are happening. We plot the mean time difference between real and predicted events for each session in the test set (see @fig-boxplot-comparison).

![Mean time difference between real and predicted events for both strategies.](images/boxplot_both_strategies.png){#fig-boxplot-comparison width=600}

In the strategy P, the *Heel-Strike* events seem to be well estimated but it not the case for the *Toe-Off* events that are predicted too late, about 150 ms after the real time of the event. This could mean that for these events, it would give better results to take one of the first points of the detected sub-phase instead of the point with the model highest probability. Since we do not have theoretical explanation of this phenomena, we can not justify the use of different procedure for either *Heel Strike* or *Toe-Off* events. On the other hand, in the strategy E, all of the event times are estimated near the real ones with no distinction between classes. For each type of event, the estimated time median is at approximately 50 ms before the real event, with a few outliers. Overall, the results given by the event detection are closer to the times given by the gold standard so we retain the strategy E. 

\newpage

## Detailed performance of the selected model

The final selected model is the neural network from the strategy E predicting directly the event times. We give a more in-depth evaluation of the final model in this part. We evaluate this model on the test set for the three metrics adapted to this strategy: the Macro-Average Sensitivity and Specificity and the GWYI (see @tbl-final-eval). We also perform a deeper comparison of the estimated event times by the model with the real ones given by the gold standard.

::: {#tbl-final-eval tbl-pos="H"}

```{r}
tibble::tibble(
  youden = c("88.7%"),
  sen = c("93.4%"),
  spe = c("96.4%")
) |>
  gt::gt() |> 
  gt::cols_label(
    youden = "Generalized Weighted Youden Index",
    sen = "MA Sensitivity",
    spe = "MA Specificity"
  ) |> 
  gt::tab_options(column_labels.background.color = "#616161")
```

Performance metrics of the selected Neural Network on the test set.

:::

For a complete comparison of our model with the gold standard, we use spatio-temporal parameters that can be computed from the four events happening during a walking cycle: the number of cycles in a session, the cycle duration and the percentage of stance phase. We compute the mean value for these parameters on each session from the test set. We differentiate the results for each foot by computing the gait cycles starting with the right and the left foot, thus we have 70 observations for this comparison (1 per session and foot in the test set). 

For the number of cycles in each session, we find the exact same number of cycles for our model and the gold standard, which is not surprising since we previously shown that the model detected the correct number of each event in every sessions. For the mean cycle duration and the mean percentage of stance phase, we use Bland-Altman plots [@bland_altman] to compare the the results (see @fig-bland-altman).

::: {#fig-bland-altman layout="[48, -4, 48]" fig-pos="H"}

![Comparison of the mean cycle duration (seconds).](images/ba_cycle_length.png){#fig-ba-cycle-length width=270}

![Comparison of the mean stance percentage (%).](images/ba_stance_percent.png){#fig-ba-stance-percent width=270}

Bland-Altman plots to compare our model with the gold standard on spatio-temporal parameters.
:::

For both parameters, the bias represented by the dashed line in the blue interval is really close to zero, meaning that our model gives similar results than the gold standard. More precisely, for the mean cycle duration, the differences between the two methods are between minus 26 ms and 21 ms, represented by the dashed lines in the red and green intervals. Since a gait cycle typically lasts one second, this difference interval seems low enough to consider our method to be comparable to the gold standard. For the mean stance percentage, the differences are between minus 8.50% and 6.5% and we know that a stance phase generally lasts about 60% of the gait cycle. Once again our model gives coherent results that are close to the parameters given by the reference method.

\newpage

# Discussion and conclusions

In this study, we used rotation data in the shape of unit quaternion time-series to analyze the hip rotation of subjects walking in a straight line. We used the GAITRite© sensor mat as the reference method to label data acquired with a wearable sensor placed at the right hip to measure rotations over time. The goal was to detect key gait events (*Heel Strike* and *Toe Off* events) in the signal to compute from them spatio-temporal parameters giving similar results as the gold standard. 

We compared two strategies to detect events during gait cycles: predicting directly the gait events or predicting sub-phases in between these events. We selected a model for both strategies, a neural network for the first one and a boosted trees model for the second one, by tuning and evaluating various classification models on different metrics: the Generalized Weighted Youden Index for event detection and accuracy for sub-phases detection. By comparing the two models with the reference events given by the gold standard GAITRite©, we chose to keep the strategy detecting directly the events as it shown more coherent results. The selected model is a neural network containing four hidden layers each having 500 nodes, with higher class weights on the minority classes of interest to address the issue of class imbalance. This model performs well on the test set as we get similar results than the ones provided by the reference method, as for the number of gait cycles, their duration, and the percentage of stance phase in each gait cycles.

Since this model was trained on healthy subjects, we expect less performance on data from patients with gait disorders as their gait would be less regular. Indeed, the model was tested on a data set containing gait data from 44 multiple sclerosis patients, each walking approximately seven meters while wearing the IMU. As we don't have reference points for this data set, the evaluation of the model's predictions was visual. For each time-serie, we plotted the predictions and rated them in three classes: perfect performance (the model detects all four events in the correct order for all the sessions), moderate performance (the model misses a few events but correctly segments the signal into cycles) and poor performance (the model misses most events). Within the 44 patients, we counted 20 perfect performances, 19 moderate performances and 5 poor performances, hence the model works perfectly for 45% of the data set and works poorly on only 11% of the data set. We can note that some of the recorded signals seem to have anomalies such as abnormal spikes, aside from the fact that unhealthy gaits are not necessarily regulars. Therefore, the results are promising.

To improve our model so it better generalizes on various subjects especially with gait disorders, we could acquire data on more subjects in various age groups. We could also use transfer learning to go from this model trained on healthy subject to a model specialized for subjects having gait disorders. Finally, we could perform rotation correction or use rotation-invariant features to counter the issue of the referential being different between subjects. 

\newpage

# References {.unnumbered}

::: {#refs}
:::


# Appendix {.unnumbered}

::: {#tbl-gaitrite-params tbl-pos="H"}

```{r}
#| layout="[45, -10, 45]"

tibble::tibble(
  param = c("Total distance (cm)",
            "Total duration (s)",
            "Speed (cm/s)",
            "Normalized speed",
            "Number of steps",
            "Walking pace (step/m)",
            "Difference of step duration (s)",
            "Difference of step length (cm)",
            "Difference of cycle duration (s)"
            ),
) |> gt::gt() |> 
  gt::cols_label(
    param = "Global Parameters",
  ) |> gt::tab_options(column_labels.background.color = "#616161")

tibble::tibble(
  param = c("Step duration (s)",
            "Cycle duration (s)",
            "Step length (cm)",
            "Cycle length (cm)",
            "Step width (cm)",
            "Simple support phase (%)",
            "Double support phase (%)",
            "Stance phase (%)",
            "Swing phase (%)",
            "Normalized step",
            "Foot rotation (°)"
            ),
) |> gt::gt() |> 
  gt::cols_label(
    param = "Bilateral Parameters",
  ) |> gt::tab_options(column_labels.background.color = "#616161")
```

Spatio-temporal parameters returned by the GAITRite© pressure mat.

:::


::: {#tbl-tuning-res-events tbl-pos="H"}

```{r}
tibble::tibble(model = c(
  rep("Multinomial Regression", 6),
  rep("Decision Tree", 6),
  rep("Bagged Trees", 6),
  rep("Random Forest", 6),
  rep("Boosted Trees", 8),
  rep("KNN", 5),
  rep("Neural Network", 7)
),
param = c(
  "spar", "n_lag", "k", "weight on positive classes", "`penalty`", "`mixture`",
  "spar", "n_lag", "k", "weight on positive classes", "`tree_depth`", "`min_n`",
  "spar", "n_lag", "k", "weight on positive classes", "`tree_depth`", "`min_n`",
  "spar", "n_lag", "k", "weight on positive classes", "`trees`", "`min_n`",
  "spar", "n_lag", "k", "weight on positive classes", "`trees`", "`min_n`", "`tree_depth`", "`learn_rate`",
  "spar", "n_lag", "k", "`neighbors`", "`dist_power`",
  "spar", "n_lag", "k", "weight on positive classes", "`hidden_units`", "`learn_rate`", "`dropout`"
),
values = c(
  "0.4", "5", "1", "80", "0.0001", "1",
  "0.3", "5", "1", "80", "5", "7",
  "0.4", "4", "1", "60", "6", "32",
  "0.6", "4", "2","80", "900", "40",
  "0.4", "4", "2", "80", "400", "40", "5", "0.01",
  "0.4", "5", "5", "5", "2",
  "0.4", "5", "1", "80", "rep(500, 4)", "0.01", "0.1"
)) |> 
  dplyr::group_by(model) |> 
  dplyr::mutate(model = ifelse(dplyr::row_number() == 1, model, "")) |> 
  dplyr::ungroup() |> 
  gt::gt() |>
  gt::tab_style(
    style = list(
      gt::cell_borders(
        sides = c("top", "bottom"),
        weight = gt::px(0)
      )),
    locations = list(
      gt::cells_body(
        columns = model,
        rows = is_empty(model)
      )
    )
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = param, rows = param %in% c("spar", "n_lag", "k"))
  ) |> 
  gt::cols_label(
    model = "Model",
    param = "Parameter",
    values = "Optimal value"
  ) |> gt::tab_options(column_labels.background.color = "#616161") |> 
  gt::fmt_markdown(columns = param)
```

Optimal parameters for event detection for each tuned model.

:::

::: {#tbl-tuning-res-phases tbl-pos="H"}

```{r}
tibble::tibble(model = c(
  rep("Multinomial Regression", 4),
  rep("Decision Tree", 4),
  rep("Bagged Trees", 4),
  rep("Random Forest", 4),
  rep("Boosted Trees", 6),
  rep("KNN", 4),
  rep("Neural Network", 5)
),
param = c(
  "spar", "n_lag", "`penalty`", "`mixture`",
  "spar", "n_lag", "`tree_depth`", "`min_n`",
  "spar", "n_lag", "`tree_depth`", "`min_n`",
  "spar", "n_lag", "`trees`", "`min_n`",
  "spar", "n_lag", "`trees`", "`min_n`", "`tree_depth`", "`learn_rate`",
  "spar", "n_lag", "`neighbors`", "`dist_power`",
  "spar", "n_lag", "`hidden_units`", "`learn_rate`", "`dropout`"
),
values = c(
  "0.4", "5", "0.0001", "1",
  "0.6", "4", "6", "13",
  "0.4", "4", "8", "40",
  "0.7", "5", "900", "2",
  "0.7", "5", "900", "3", "12", "0.1",
  "0.7", "5", "10", "1",
  "0.4", "4", "rep(500, 3)", "0.1", "0.1"
)) |> 
  dplyr::group_by(model) |> 
  dplyr::mutate(model = ifelse(dplyr::row_number() == 1, model, "")) |> 
  dplyr::ungroup() |> 
  gt::gt() |>
  gt::tab_style(
    style = list(
      gt::cell_borders(
        sides = c("top", "bottom"),
        weight = gt::px(0)
      )),
    locations = list(
      gt::cells_body(
        columns = model,
        rows = is_empty(model)
      )
    )
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = param, rows = param %in% c("spar", "n_lag"))
  ) |> gt::cols_label(
    model = "Model",
    param = "Parameter",
    values = "Optimal value"
  ) |> gt::tab_options(column_labels.background.color = "#616161") |> 
  gt::fmt_markdown(columns = param)
```

Optimal parameters for phase detection for each tuned model.

:::
