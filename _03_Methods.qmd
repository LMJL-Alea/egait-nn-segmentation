# Methods {#sec-methods}

## Classification strategies {#sec-classification-strategies}

In this work, the objective is to identify which timepoints of unit QTS correspond to the RHS, LTO, LHS and RTO events. For this purpose, we consider the timepoints as statistical units (observations) and we aim at labelling them by means of classification models. In this view, we can first create the data set that we will use for training. The following code achieves this task by binding together all timepoints from all walking sessions while attaching to each timepoint:

- an `event_type` which affects it to one of the five gait events defined in @sec-gaitrite-data;
- a `phase_type` which affects it one of the four gait pahses defined in @sec-gaitrite-data.

```{r}
events_to_phases <- function(events) {
  events_of_interest <- events != "None"
  first_event <- events[events_of_interest][1]
  phase_durations <- diff(c(0, sort(which(events_of_interest))))
  n_phases <- length(phase_durations)
  phase_names <- switch(
    first_event,
    "RHS" = rep(
      c("Swing", "Pre-Stance", "Stance", "Pre-Swing"),
      times = n_phases
    )[1:n_phases],
    "LTO" = rep(
      c("Pre-Stance", "Stance", "Pre-Swing", "Swing"),
      times = n_phases
    )[1:n_phases],
    "LHS" = rep(
      c("Stance", "Pre-Swing", "Swing", "Pre-Stance"),
      times = n_phases
    )[1:n_phases],
    "RTO" = rep(
      c("Pre-Swing", "Swing", "Pre-Stance", "Stance"),
      times = n_phases
    )[1:n_phases]
  )
  purrr::map2(
    phase_names,
    phase_durations,
    \(phase_name, phase_duration) rep(phase_name, times = phase_duration)
  ) |>
    purrr::list_c()
}

labelled_gait_data <- purrr::map(1:nrow(bhg), \(session_index) {
  gaitrite_data <- gaitrite_data |>
    dplyr::filter(session == session_index) |>
    dplyr::select(-session)
  imu_data[[session_index]] |>
    dplyr::left_join(gaitrite_data, by = c("time" = "event_time")) |>
    dplyr::mutate(
      event_type = dplyr::if_else(is.na(event_type), "None", event_type),
      phase_type = events_to_phases(event_type)
    )
}) |>
  dplyr::bind_rows(.id = "session") |>
  dplyr::mutate(
    session = as.numeric(session),
    event_type = factor(
      event_type,
      levels = c("RHS", "LTO", "LHS", "RTO", "None")
    ),
    phase_type = factor(
      phase_type,
      levels = c("Pre-Stance", "Stance", "Pre-Swing", "Swing")
    )
  )
class(labelled_gait_data) <- class(labelled_gait_data)[-1]
head(labelled_gait_data)
```

We first need to decide what models should predict. In effect, we can adopt two different strategies.

The most straightforward way pertains to predicting the gait events of interest themselves. We call it **Strategy E**, where **E** stands for [E]{.underline}vents. Following this strategy, this means that we must design a multiclass prediction model with 5 classes (RHS, LTO, LHS, RTO and None) as defined in @sec-gaitrite-data. The first four events (RHS, LTO, LHS and RTO) are coined *events of interest* while the last one encodes the so-called *negative* class. While conveniently aiming at directly predicting the occurrence of gait events of interest, this strategy suffers from a severe class imbalance issue, with the *None* (negative) class being widely over-represented as shown by @tbl-e-counts.

A solution to mitigate this severe class imbalance issue is to predict gait phases instead of events at the cost of some post-processing efforts needed to identify the occurences of RHS, LTO, LHS and RTO after the phase prediction step. As defined in @sec-gaitrite-data, there are four phases to predict (pre-stance, stance, pre-swing and swing). We call this strategy **Strategy P**, where **P** stands for [P]{.underline}hases. @tbl-p-counts exhibits the frequency of timepoints in each phase, which demonstrate that this strategy successfully reduces dramatically class imbalance.

```{r}
#| label: tbl-class-imbalance
#| tbl-cap: "Strategy E: Count and proportion of observations in each class."
#| tbl-pos: "H"
tibble::tibble(
  class = c(
    "Right Heel Strike",
    "Left Toe Off",
    "Left Heel Strike",
    "Right Toe Off",
    "None"
  ),
  nb_obs = c("973", "1004", "994", "982", "158401"),
  prop = c("0.60%", "0.62%", "0.61%", "0.60%", "97.57%")
) |>
  gt::gt() |>
  gt::cols_label(
    class = "Class",
    nb_obs = "Number of observations",
    prop = "Proportion"
  ) |>
  gt::cols_align(align = "center") |>
  gt::tab_style(
    style = list(gt::cell_text(style = "italic")),
    locations = gt::cells_body(columns = class)
  ) |>
  gt::tab_options(column_labels.background.color = "#616161")
```

[AST] TO MODIFY

Finally, the following code creates the labelled data set that we will use to elaborate the feature space and produces @tbl-class-summary which exhibits class frequencies whether we focus on gait events () or gait phases ().

```{r}
#| label: tbl-class-summary
#| tbl-cap: Two tables
#| tbl-subcap: ["mtcars", "Just cars"]
#| layout-ncol: 2
#| html-table-processing: none
labelled_gait_data |>
  dplyr::count(event_type) |>
  gt::gt() |>
  gt::cols_label(
    event_type = "Event",
    n = "Frequency"
  ) |>
  gt::opt_stylize(style = 6, color = 'gray') |>
  gt::cols_align(align = "center") |>
  gt::tab_style(
    style = "vertical-align:top",
    locations = gt::cells_column_labels()
  )

labelled_gait_data |>
  dplyr::count(phase_type) |>
  gt::gt() |>
  gt::cols_label(
    phase_type = "Phase",
    n = "Frequency"
  ) |>
  gt::opt_stylize(style = 6, color = 'gray') |>
  gt::cols_align(align = "center") |>
  gt::tab_style(
    style = "vertical-align:top",
    locations = gt::cells_column_labels()
  )
```

We can notice that gait events of interest are largely under-represented while this class imbalance issue is moderate when we put the focus on gait phases.

Another way of showing this issue is to overlay the occurence of the events of interest detected by the GAITRiteÂ® mat on a QTS recorded by the IMU device (see @fig-event-timeserie). Each colored point represents an event of interest and all other times belong to the *None* class.

![Gait event occurences overlaid on a unit QTS.](images/events_on_time_serie_MSI_N_R2.png){#fig-event-timeserie width=350}

To mitigate this effect, when the mat records the occurence of a gait event of interest at a specific time point, we also label surrounding time points with the same event. The size of this window is controlled by a parameter $k$ which sets the number of preceding points labelled as the event of interest. It also allows to account for some uncertainty as the time range between two points is only 10 ms. For instance, if we set $k = 1$, we label $3$ observations as part of the event rather than just one. In this case, we consider that the event happens in a window of $20$ ms.

Strategy P: Predicting gait [P]{.underline}hases

: The strategy P pertains to predicting gait phases in the signal that are characterized by the gait events. We label each part in the signal between two events as a phase, leading to four classes for our observations:

- *Pre-Stance* phase: between *Right Heel Strike* and *Left Toe Off*,
- *Stance* phase: between *Left Toe Off* and *Left Heel Strike*,
- *Pre-Swing* phase: between *Left Heel Strike* and *Right Toe Off*,
- *Swing* phase: between *Right Toe Off* and *Right Heel Strike*.

The definition of these phases matches a typical gait cycle starting when the right foot hits the ground. This strategy generates prediction classes that are less imbalanced, as opposed to the first strategy in which the *None* class dominates the dataset. The downside is that some post-processing is needed because the segmentation models will predict gait phases and we need to further decide on the exact orrucence of gait events of interest within the predicted phases.

![Gait phases overlaid on a unit QTS.](images/phases_on_time_serie_MSI_N_R2.png){#fig-phases-timeserie width=350 fig-pos="H"}

@fig-phases-timeserie shows the gait phases overlaid on a QTS. Each gait phase is mapped to a suitable color. We can see that some phases are larger than others: the *pre-stance* and *pre-swing* phases are indeed shorter than the *stance* and *swing* phases. However, class imbalance is not as severe as in the first strategy.

## Supervised classification models {#sec-supervised-classification-models}

In addition to the two classification strategies presented in @sec-classification-strategies, the literature presents a wide variety of machine learning models that can be used for supervised classification. In this section, we detail the different models that we selected to compare for our task of gait event detection and phase prediction. The choice of these models was made among the models available in the [{tidymodels}](https://www.tidymodels.org) framework in R [@tidymodels] in order to guarantee a consistent and identical implementation for their training and evaluation.

Multinomial Regression [@hosmer2013applied]

: A *multinomial regression* is built by fitting a set of independent binary logistic regressions. We want to classify our observations in $K$ classes. We choose one category to be the baseline and name it $k_0$. We build $K-1$ logit equations modeling the log-odds of a class $k$ versus the class $k_0$. The logit equation for a class $k$ is:

$$
\log \left( \frac{\mathbb{P}(Y = k | X)}{\mathbb{P}(Y = k_0 | X)} \right) = \beta_{0k} + \beta_k^\top X,
$$ {#eq-logit}

with $Y$ the class of the observation, $X$ the predictors, $\beta_{0k}$ the intercept and $\beta_{1k}, \dots, \beta_{pk}$ the regression coefficients.

The softmax function is then used to convert the $K-1$ logit scores into probabilities for each class. Finally, the observation is classified in the class with the highest probability. To fit the model, ie to find the best values for the coefficients, we maximize the log likelihood function.

When we have a large set of variables, we can regularized the estimation by introducing penalty terms in the log-likelihood function [@hastie2015statistical]. The two main types of penalization are the L1 regularization (also called a Lasso model) and the L2 regularization (also called a Ridge model). The Ridge regression prevents overfitting by adding the term $\lambda \sum_{k=1}^K \beta_k^2$ to the loss function, while the Lasso model selects features by adding the term $\lambda \sum_{k=1}^K |\beta_k|$ to the loss, giving a sparse solution with only some non-zero coordinates. The parameter $\lambda$ is called the regularization parameter.

Decision tree [@kuhn2013applied; @breiman2017classification]

: *Decision trees* consist of a nested sequence of if-then statements for the predictor classifying data. Observations are assigned to their class according to the variable values. 

The goal is to classify the observations into smaller and more homogeneous groups, forming rectangular areas within data points. Homogeneity is defined here as how pure the splitting nodes are , meaning that there is a higher proportion of one class in each node. We can use the Gini index to compute purity. In a scenario with $K$ classes, each having probability $p_1, \dots, p_K$, the Gini impurity for a certain node is defined as:

$$
G(p) = \sum_{k=1}^K p_k \sum_{j \ne k}^K p_j = \sum_{k=1}^K p_k (1-p_k) = 1 - \sum_{k=1}^K (p_k)^2.
$$ {#eq-gini}

This index is minimal when one of the probabilities tends toward zero, indicating that the node is pure. To build the tree, the model builds splitting nodes by evaluating each splitting point (ie values taken by variables to split observations into groups) to find the one minimizing the Gini index, until a stopping criteria is met such as a maximum tree depth.

Bagged trees, Random forest and Boosted trees [@kuhn2013applied]

: A *bagged trees* model is an ensemble of $M$ decision trees trained in parallel. Each decision tree is built with a bootstrap sample of the original data, which is a sample of same size than the original data by selecting random observations with replacement. Then, each of these trees is used to generate a prediction for a new observation. Finally, the observation is classified in the class that has collected the greatest number of votes from all the trees.

A *random forest* is a similar model, also using a number of decision trees. The model also trains a number of trees in parallel on bootstrap samples of the original data. The difference is that, at each split, a random subset of variables is selected to find the best predictors within this subset. The observations are then classified as usual. The prediction is done as seen before, with each tree voting for a class and selecting the majority to classify the observation.

Finally, *boosted trees* train a number of decision trees sequentially to learn from the previous tree mistakes and put a higher weight on previously misclassified observations. The trees are also trained on bootstrapped samples, selecting a random subset of variables at each node. The final prediction is the weighted sum of the predictions from each tree with the weights determined by the performance of the trees.

$k$-Nearest Neighbors [@cover1967nearest]

: A *$k$-Nearest Neighbors* model finds the $k$ closest data points to a given input and makes prediction based on the majority class within its neighbors. This model is non-parametric as it does not make assumption about the underlying data.

To calculate the distance between the input and its neighbors, the Minkowski distance can be used. It is defined as:

$$
d(x, y) = \left[ \sum_{i=1}^n (x_i-y_i)^p \right]^{1/p}.
$$ {#eq-minkowski}

This distance is a generalization of multiple well-know distances as we find the Euclidean distance when $p = 2$ and the Manhattan distance when $p = 1$.

Neural networks [@varma2018]

: Neural Networks are deep learning models inspired by the human brain. They consist of interconnected nodes organized in layers which process the data by passing it from one layer to the next. The model contains two essential parameters: weights on each connection and biases on each node. The input data is given to the input layer and then passes through the hidden layers. At each layer, a pre-activation is computed with the weights and biases and is transformed as an activation with non-linear functions (usually the function *ReLU*). Finally, the result goes into the output layer which uses an output function (the *softmax* function for more than two classes) to return the predicted class for each observation. It learns data pattern by adjusting the parameters during forward and backward propagation for each observation to minimize a loss that represents the difference between real and predicted values. Most often, the loss used is the cross-entropy which is defined as followed for the observation $m$:

$$
\square(m) = - \sum_{k=1}^K t_k(m) \log (y_k(m)),
$$ {#eq-crossentropy}

with $t_k$ the ground truth and $y_k$ the classification probability.

These models are complex and rely on a efficient algorithm for the parameter update and an adapted activation function, as well as possible utilization of batch normalization, regularization or dropout to find a model with a good complexity for the data. Tuning is done to find the best hyper-parameters such as the number of layers and nodes or the learning rate.

## Dealing with class imbalance {#sec-class-imbalance}

Classification strategy E leads to a strong class imbalance as the *None* class contains most of the observations. Even strategy P has some imbalance between the classes, although less severe. This is an important issue in machine learning because, in such situations, models learn quite well to predict the majority class which, in our application, is not of great interest, but fail to correctly predict the minority classes which are the ones we want to detect. In addition, performance metrics such as accuracy are not adapted to evaluate models in this context because a model predicting only the majority class would achieve a high accuracy while being useless.

A lot of sampling algorithms exist to mitigate this issue such as random oversampling or random undersampling which add or remove observations from certain classes to create a more balanced dataset. More complex methods exist, among which the popular SMOTE [@chawla2002] or the ADASYN [@haibo2008] algorithms that creates synthetic data by considering the classes of the nearest neighbors of random observations. Although these algorithms are very popular, they did not perform well in our application. We conjecture that this is due to the fact that our data is too imbalanced for these methods.

To tell the model to pay more attention to the patterns in the minority classes, an effective way is to put a larger weight on the observations belonging to such classes compared. More precisely, the weight given to each observation specifies how much this observation influences the model's estimation. In order for the model to be biased towards observations deemed more important for the application at hand (minority classes in our case), the weights of the observations are integrated into the cost function. Weights naturally impact the cost of misclassification in the sense that misclassifying more important observations should cost more, encouraging the model to avoid these types of mistakes [@hashemi2018].

When the classes are not too imbalanced, such as in Strategy P (phase prediction), we can use the inverse class frequency weights [@huang2016learning] where the weight on the class $k$ is given by:

$$
\omega_k = \frac{n}{n_\mathrm{classes} \hspace{1mm} n_k}
$$ {#eq-inv-class-frequency}

with $n$ the total number of observations, $n_\mathrm{classes}$ the number of classes and $n_k$ the number of observations in the class $k$. This formula yields balanced weights but we can also choose class weights manually and tune them.

## Performance metrics {#sec-performance-metrics}

To tune and evaluate the models, we need to use appropriate metrics according to the classification strategy and the amount of class imbalance.

### Metrics for Strategy P (Phase prediction)

In this strategy, we aim at predicting phases. All classes are therefore equally important and they are not severely unbalanced. Consequently, we use the accuracy metric to tune and evaluate models under this paradigm. It is the most widely used metric in machine learning and is defined as the number of correct predictions divided by the total number of predictions.

### Metrics for Strategy E (Event prediction)

In this strategy, we aim at predicting occurences of gait events directly. As a result, four of the five classes are the classes of interest while the majority class is not relevant. Furthermore, the majority class contains the large majority of observations (for instance, $97.57\%$ of the observations for $k=0$ and $92.70\%$ of the observations for $k=1$), which represents a case of severe class imbalance. Accuracy is not adapted in this case because a model which always predicts the majority class will be deemed a *good* model in the sense of accuracy, while completely missing the classes of interest. We therefore need to use other metrics that are adapted to this situation.

An important tool for designing performance metrics of classification methods is the *confusion matrix* $C$ which is a $K \times K$ matrix for a $K$-class classification problem such that the element $c_{i,j}$ represents the number of observations from the class $i$ that have been predicted in the class $j$. This matrix summarizes the performance of a classification model by showing how many observations have been correctly or incorrectly classified in each class. In a binary classification problem, the matrix reports the number of true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN). Two of the metrics that can be calculated from this matrix are known as *sensitivity* and *specificity*. The sensitivity Se = TP / (TP + FN) measures the proportion of accurate predictions for the positive class among all actual positive instances. Hence, low sensitivity indicates that the model is missing many instances of the positive class. Conversely, the specificity Sp = TN / (TN + FP) measures the proportion of accurate predictions for the negative class among all actual negative instances. In scenarios with class imbalance, sensitivity is particularly crucial as it reveals how effectively the model identifies instances of the minority class, which is often designated as the positive class.

In a multiclass classification problem, we can extend these metrics to use them with more than one positive class. For detecting gait events of interest, we have four classes of interest which correspond to the minority classes compared to time points at which no relevant gait event occurs. We therefore define a $5 \times 5$ confusion matrix as exhibited in @fig-confusion-matrix. The element $c_{i,j}$ is the number of observations from the class $i$ that has been predicted in the class $j$. Therefore, $c_{i,\cdot}$ represents the number of observations from the class $i$, while $c_{\cdot,j}$ represents the number of observations that the model predicted to be in the class $j$.

![Confusion matrix for five classes including the *None* one represented by the letter N and four classes of interest named P1 to P4.](images/confusion-matrix.png){#fig-confusion-matrix width=300 pos="H"}

In this work, we use generalizations ofsensibility and specificity coined macro-average sensibility and macro-average specificity [@mortaz2020]. For a $K$-classifcation problem where the $K$-*th* class is not of interest, these metrics read:

$$
\mathrm{maSe} = \frac{1}{K-1} \sum_{i=1}^{K-1} \frac{c_{i,i}}{c_{i,\cdot}}, \quad \mathrm{maSp} = \frac{1}{K} \sum_{i=1}^K \frac{\sum_{j \ne k \ne i} c_{j,k} + \sum_{j \ne i} c_{j,j}}{\sum_{j \ne i} c_{\cdot,j}}.
$$ {#eq-ma-metrics}

Tuning a model is achieved by optimizing a single metric. We therefore need to combine the information from the MA Sensitivity and MA Specificity into a single metric. The weighted Youden index [@li2013weighted] has been proposed in binary classification problems which is a convex combination of sensitivity and specificity. We generalize the weighted Youden index to multiclass classification problems by plugging in the macro-average versions of sensitivity and specificity. The resulting generalized weighted Youden index (GWYI) is defined as:

$$
\mathrm{GWYI} = 2(w \times \mathrm{maSe} + (1-w) \times \mathrm{maSp}) - 1,
$$ {#eq-youden}

with $w \in [0,1]$ representing the importance given to the accurate prediction of the minority classes. In our application, we chose to put a weight of $0.7$ on the macro-average sensitivity to favor a good proportions of predictions of minority classes at the cost of over predicting those classes.

## Data splitting and tuning strategy

To correctly tune and evaluate the model, data is initially split into two sets: the *training set* in which we put 80% of the observations and the *test set* in which we put 20% of observations. The critical point is to never use the test set during training or tuning to be able to evaluate the trained and tuned model on data that it never used for training or tuning. We chose to keep entire walking sessions in each set so that the model would never see any observations coming from sessions in the test set during training. With this strategy, we included 139 sessions in the training set and 35 sessions in the test set.

For some models such as neural networks, the training set is further divided into a train and a validation set, containing 80% and 20% of the training set respectively. The model computes the predictions on the train set and the loss is evaluated on the validation set to monitor the training process and avoid overfitting.

For tuning, the training set is divided into five *folds* which are random partitions of the training set to get five equally-sized subsets. In each fold, data is then split into a train and an evaluation set, containing 90% and 10% of the data respectively (see @fig-datasplitting). The strategy of $k$-fold cross-validation for tuning models is particularly effective because it provides $k$ estimates of the performance metric on which models are tuned which reduces the variance of the estimate compared to a single train/evaluation split.

![A schematic view of the data splitting process.](images/data-splitting.png){#fig-datasplitting width=600}

To tune hyper-parameters, automated tuning was used by grid search which is an exhaustive search in the hyper-parameter space. For each hyper-parameter, we defined a grid of plausible values. Then, for each combination of such values, we evaluated the performance metric of interest (see @sec-performance-metrics) and chose the best possible combination of hyper-parameters as the one maximizing the metric. For each combination of hyper-parameters, the performance metric was computed by averaging the metric over the five folds of cross-validation.